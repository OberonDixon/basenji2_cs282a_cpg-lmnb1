{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from natsort import natsorted\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.enable_eager_execution()\n",
    "\n",
    "from basenji import blocks\n",
    "from basenji import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFR_INPUT = 'sequence'\n",
    "TFR_OUTPUT = 'target'\n",
    "\n",
    "seq_length = 262144\n",
    "target_length = 2048\n",
    "num_targets = 1312\n",
    "\n",
    "def file_to_records(filename):\n",
    "    return tf.data.TFRecordDataset(filename, compression_type='ZLIB')\n",
    "\n",
    "def generate_parser(raw=False):\n",
    "\n",
    "    def parse_proto(example_protos):\n",
    "        \"\"\"Parse TFRecord protobuf.\"\"\"\n",
    "\n",
    "        features = {\n",
    "            TFR_INPUT: tf.io.FixedLenFeature([], tf.string),\n",
    "            TFR_OUTPUT: tf.io.FixedLenFeature([], tf.string)\n",
    "        }\n",
    "        parsed_features = tf.io.parse_single_example(example_protos, features=features)\n",
    "\n",
    "        sequence = tf.io.decode_raw(parsed_features[TFR_INPUT], tf.uint8)\n",
    "        if not raw:\n",
    "            sequence = tf.reshape(sequence, [seq_length, 4])\n",
    "            sequence = tf.cast(sequence, tf.float32)\n",
    "\n",
    "        targets = tf.io.decode_raw(parsed_features[TFR_OUTPUT], tf.float16)\n",
    "        if not raw:\n",
    "            targets = tf.reshape(targets, [target_length, num_targets])\n",
    "            targets = tf.cast(targets, tf.float32)\n",
    "\n",
    "        return sequence, targets\n",
    "\n",
    "    return parse_proto\n",
    "\n",
    "def make_dataset(tfr_pattern, batch_size, mode):\n",
    "    \"\"\"Make Dataset w/ transformations.\"\"\"\n",
    "\n",
    "    # initialize dataset from TFRecords glob\n",
    "    tfr_files = natsorted(glob.glob(tfr_pattern))\n",
    "    dataset = tf.data.Dataset.list_files(tf.constant(tfr_files), shuffle=False)\n",
    "\n",
    "    # train\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        # repeat\n",
    "        dataset = dataset.repeat()\n",
    "\n",
    "    # flat mix files\n",
    "    dataset = dataset.flat_map(file_to_records)\n",
    "    \n",
    "    # parse\n",
    "    dataset = dataset.map(generate_parser())\n",
    "\n",
    "    # batch\n",
    "    dataset = dataset.batch(batch_size)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfr_train_full = 'data/tfrecords/train-0.tfr'\n",
    "train_data = make_dataset(tfr_train_full, 2, tf.estimator.ModeKeys.TRAIN) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfr_eval_full = 'data/tfrecords/valid-0.tfr'\n",
    "eval_data = make_dataset(tfr_eval_full, 2, tf.estimator.ModeKeys.EVAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model():\n",
    "    sequence = tf.keras.Input(shape=(seq_length, 4), name='sequence')\n",
    "    current = sequence\n",
    "\n",
    "    current = blocks.conv_tower(current, filters_init=64, repeat=7, kernel_size=5,\n",
    "                     activation='relu', pool_size=2,\n",
    "                     batch_norm=True, bn_momentum=0.99)\n",
    "\n",
    "    preds = blocks.dense(current, units=1312)\n",
    "\n",
    "    return tf.keras.Model(inputs=sequence, outputs=preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 2\n",
    "train_batches = 32\n",
    "valid_batches = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer1 = tf.keras.optimizers.SGD(learning_rate=.005, momentum=0.99)\n",
    "\n",
    "# model1 = make_model()\n",
    "# model1.compile(loss='poisson', optimizer=optimizer1,\n",
    "#                metrics=[metrics.PearsonR(num_targets)])\n",
    "\n",
    "# model1.fit(train_data, epochs=num_epochs, steps_per_epoch=train_batches,\n",
    "#            validation_data=eval_data, validation_steps=valid_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model1.evaluate(eval_data, steps=valid_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - 96s - train_loss: 0.9996 - train_r: 0.0008 - valid_loss: 1.0204 - valid_r: 0.0014\n",
      "Epoch 1 - 87s - train_loss: 0.9631 - train_r: 0.0021 - valid_loss: 0.8511 - valid_r: 0.0063\n"
     ]
    }
   ],
   "source": [
    "optimizer2 = tf.keras.optimizers.SGD(learning_rate=.005, momentum=0.99)\n",
    "\n",
    "model2 = make_model()\n",
    "model2.compile(loss='poisson', optimizer=optimizer2,\n",
    "               metrics=[metrics.PearsonR(num_targets)])\n",
    "               \n",
    "loss_fn = tf.keras.losses.Poisson()\n",
    "train_loss = tf.keras.metrics.Poisson()\n",
    "train_r = metrics.PearsonR(num_targets)\n",
    "valid_r = metrics.PearsonR(num_targets)\n",
    "\n",
    "@tf.function\n",
    "def train_step(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        pred = model2(x, training=True)\n",
    "        loss = loss_fn(y, pred)\n",
    "    train_loss(y, pred)\n",
    "    train_r(y, pred)\n",
    "    gradients = tape.gradient(loss, model2.trainable_variables)\n",
    "    optimizer2.apply_gradients(zip(gradients, model2.trainable_variables))\n",
    "\n",
    "for ei in range(num_epochs):\n",
    "    # train\n",
    "    t0 = time.time()\n",
    "    si = 0\n",
    "    for x, y in train_data:\n",
    "        train_step(x, y)\n",
    "        si += 1\n",
    "        if si >= train_batches:\n",
    "            break\n",
    "\n",
    "    train_loss_epoch = train_loss.result().numpy()\n",
    "    train_r_epoch = train_r.result().numpy()\n",
    "    print('Epoch %d - %ds - train_loss: %.4f - train_r: %.4f' % \\\n",
    "          (ei, (time.time()-t0), train_loss_epoch, train_r_epoch), end='')\n",
    "\n",
    "    # valid\n",
    "    valid_loss, valid_pr = model2.evaluate(eval_data, steps=valid_batches, verbose=0)\n",
    "    print(' - valid_loss: %.4f - valid_r: %.4f' % (valid_loss, valid_pr), flush=True)\n",
    "\n",
    "    # reset\n",
    "    train_loss.reset_states()\n",
    "    train_r.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
