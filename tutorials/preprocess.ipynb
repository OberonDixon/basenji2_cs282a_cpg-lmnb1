{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing a dataset for Basenji training involves a series of design choices.\n",
    "\n",
    "The input you bring to the pipeline is:\n",
    "* BigWig coverage tracks\n",
    "* Genome FASTA file\n",
    "\n",
    "First, make sure you have an hg19 FASTA file visible. If you have it already, put a symbolic link into the data directory. Otherwise, I have a machine learning friendly simplified version you can download in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, subprocess\n",
    "\n",
    "if not os.path.isfile('data/hg19.ml.fa'):\n",
    "    subprocess.call('curl -o data/hg19.ml.fa https://storage.googleapis.com/basenji_tutorial_data/hg19.ml.fa', shell=True)\n",
    "    subprocess.call('curl -o data/hg19.ml.fa.fai https://storage.googleapis.com/basenji_tutorial_data/hg19.ml.fa.fai', shell=True)                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's grab a few CAGE datasets from FANTOM5 related to heart biology.\n",
    "\n",
    "These data were processed by\n",
    "1. Aligning with Bowtie2 with very sensitive alignment parameters.\n",
    "2. Distributing multi-mapping reads and estimating genomic coverage with [bam_cov.py](https://github.com/calico/basenji/blob/master/bin/bam_cov.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.isfile('data/CNhs11760.bw'):\n",
    "    subprocess.call('curl -o data/CNhs11760.bw https://storage.googleapis.com/basenji_tutorial_data/CNhs11760.bw', shell=True)\n",
    "    subprocess.call('curl -o data/CNhs12843.bw https://storage.googleapis.com/basenji_tutorial_data/CNhs12843.bw', shell=True)\n",
    "    subprocess.call('curl -o data/CNhs12856.bw https://storage.googleapis.com/basenji_tutorial_data/CNhs12856.bw', shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we'll write out these BigWig files and labels to a samples table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lines = [['index','identifier','file','clip','sum_stat','description']]\n",
    "lines.append(['0', 'CNhs11760', 'data/CNhs11760.bw', '384', 'sum', 'aorta'])\n",
    "lines.append(['1', 'CNhs12843', 'data/CNhs12843.bw', '384', 'sum', 'artery'])\n",
    "lines.append(['2', 'CNhs12856', 'data/CNhs12856.bw', '384', 'sum', 'pulmonic_valve'])\n",
    "\n",
    "samples_out = open('data/heart_wigs.txt', 'w')\n",
    "for line in lines:\n",
    "    print('\\t'.join(line), file=samples_out)\n",
    "samples_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to choose genomic sequences to form batches for stochastic gradient descent, divide them into training/validation/test sets, and construct TFRecords to provide to downstream programs.\n",
    "\n",
    "The script [basenji_data.py](https://github.com/calico/basenji/blob/master/bin/basenji_data.py) implements this procedure.\n",
    "\n",
    "The most relevant options here are:\n",
    "\n",
    "| Option/Argument | Value | Note |\n",
    "|:---|:---|:---|\n",
    "| -d | 0.1 | Down-sample the genome to 10% to speed things up here. |\n",
    "| -g | data/unmap_macro.bed | Dodge large-scale unmappable regions like assembly gaps. |\n",
    "| -l | 131072 | Sequence length. |\n",
    "| --local | True | Run locally, as opposed to on my SLURM scheduler. |\n",
    "| -o | data/heart_l131k | Output directory |\n",
    "| -p | 8 | Uses multiple concourrent processes to read/write. |\n",
    "| -t | .1 | Hold out 10% sequences for testing. |\n",
    "| -v | .1 | Hold out 10% sequences for validation. |\n",
    "| -w | 128 | Pool the nucleotide-resolution values to 128 bp bins. |\n",
    "| fasta_file| data/hg19.ml.fa | FASTA file to extract sequences from. |\n",
    "| targets_file | data/heart_wigs.txt | Target samples table with BigWig paths. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contigs divided into\n",
      " Train:  4701 contigs, 2169074921 nt (0.8005)\n",
      " Valid:   572 contigs,  270358978 nt (0.0998)\n",
      " Test:    584 contigs,  270330829 nt (0.0998)\n",
      "/Users/davidkelley/code/Basenji/bin/basenji_data.py:239: FutureWarning: read_table is deprecated, use read_csv instead, passing sep='\\t'.\n",
      "  targets_df = pd.read_table(targets_file, index_col=0)\n",
      "basenji_data_read.py -w 128 -u sum -c 384.000000 -s 1.000000 data/CNhs11760.bw data/heart_l131k/sequences.bed data/heart_l131k/seqs_cov/0.h5 &> data/heart_l131k/seqs_cov/0.err\n",
      "basenji_data_read.py -w 128 -u sum -c 384.000000 -s 1.000000 data/CNhs12843.bw data/heart_l131k/sequences.bed data/heart_l131k/seqs_cov/1.h5 &> data/heart_l131k/seqs_cov/1.err\n",
      "basenji_data_read.py -w 128 -u sum -c 384.000000 -s 1.000000 data/CNhs12856.bw data/heart_l131k/sequences.bed data/heart_l131k/seqs_cov/2.h5 &> data/heart_l131k/seqs_cov/2.err\n",
      "basenji_data_write.py -s 0 -e 256 data/hg19.ml.fa data/heart_l131k/sequences.bed data/heart_l131k/seqs_cov data/heart_l131k/tfrecords/train-0.tfr &> data/heart_l131k/tfrecords/train-0.err\n",
      "basenji_data_write.py -s 256 -e 512 data/hg19.ml.fa data/heart_l131k/sequences.bed data/heart_l131k/seqs_cov data/heart_l131k/tfrecords/train-1.tfr &> data/heart_l131k/tfrecords/train-1.err\n",
      "basenji_data_write.py -s 512 -e 768 data/hg19.ml.fa data/heart_l131k/sequences.bed data/heart_l131k/seqs_cov data/heart_l131k/tfrecords/train-2.tfr &> data/heart_l131k/tfrecords/train-2.err\n",
      "basenji_data_write.py -s 768 -e 1024 data/hg19.ml.fa data/heart_l131k/sequences.bed data/heart_l131k/seqs_cov data/heart_l131k/tfrecords/train-3.tfr &> data/heart_l131k/tfrecords/train-3.err\n",
      "basenji_data_write.py -s 1024 -e 1280 data/hg19.ml.fa data/heart_l131k/sequences.bed data/heart_l131k/seqs_cov data/heart_l131k/tfrecords/train-4.tfr &> data/heart_l131k/tfrecords/train-4.err\n",
      "basenji_data_write.py -s 1280 -e 1499 data/hg19.ml.fa data/heart_l131k/sequences.bed data/heart_l131k/seqs_cov data/heart_l131k/tfrecords/train-5.tfr &> data/heart_l131k/tfrecords/train-5.err\n",
      "basenji_data_write.py -s 1499 -e 1679 data/hg19.ml.fa data/heart_l131k/sequences.bed data/heart_l131k/seqs_cov data/heart_l131k/tfrecords/valid-0.tfr &> data/heart_l131k/tfrecords/valid-0.err\n",
      "basenji_data_write.py -s 1679 -e 1858 data/hg19.ml.fa data/heart_l131k/sequences.bed data/heart_l131k/seqs_cov data/heart_l131k/tfrecords/test-0.tfr &> data/heart_l131k/tfrecords/test-0.err\n"
     ]
    }
   ],
   "source": [
    "! basenji_data.py -d .1 -g data/unmap_macro.bed -l 131072 --local -o data/heart_l131k -p 8 -t .1 -v .1 -w 128 data/hg19.ml.fa data/heart_wigs.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, data/heart_l131k contains relevant data for training.\n",
    "\n",
    "*contigs.bed* contains the original large contiguous regions from which training sequences were taken (possibly strided).\n",
    "*sequences.bed*\n",
    "\n",
    "contains the train/valid/test sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 179 test\n",
      "1499 train\n",
      " 180 valid\n"
     ]
    }
   ],
   "source": [
    "! cut -f4 data/heart_l131k/sequences.bed | sort | uniq -c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chr22\t30113858\t30244930\ttrain\n",
      "chr8\t76979074\t77110146\ttrain\n",
      "chr3\t89515975\t89647047\ttrain\n"
     ]
    }
   ],
   "source": [
    "! head -n3 data/heart_l131k/sequences.bed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chr9\t132920986\t133052058\tvalid\n",
      "chr9\t47160133\t47291205\tvalid\n",
      "chr13\t79498687\t79629759\tvalid\n"
     ]
    }
   ],
   "source": [
    "! grep valid data/heart_l131k/sequences.bed | head -n3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chr14\t55143579\t55274651\ttest\n",
      "chr3\t4825239\t4956311\ttest\n",
      "chr19\t48710409\t48841481\ttest\n"
     ]
    }
   ],
   "source": [
    "! grep test data/heart_l131k/sequences.bed | head -n3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--  1 davidkelley  staff  10603899 May 31 17:22 data/heart_l131k/tfrecords/test-0.tfr\n",
      "-rw-r--r--  1 davidkelley  staff  15181264 May 31 17:22 data/heart_l131k/tfrecords/train-0.tfr\n",
      "-rw-r--r--  1 davidkelley  staff  15182578 May 31 17:22 data/heart_l131k/tfrecords/train-1.tfr\n",
      "-rw-r--r--  1 davidkelley  staff  15183867 May 31 17:22 data/heart_l131k/tfrecords/train-2.tfr\n",
      "-rw-r--r--  1 davidkelley  staff  15170009 May 31 17:22 data/heart_l131k/tfrecords/train-3.tfr\n",
      "-rw-r--r--  1 davidkelley  staff  15156848 May 31 17:22 data/heart_l131k/tfrecords/train-4.tfr\n",
      "-rw-r--r--  1 davidkelley  staff  12965624 May 31 17:22 data/heart_l131k/tfrecords/train-5.tfr\n",
      "-rw-r--r--  1 davidkelley  staff  10687668 May 31 17:22 data/heart_l131k/tfrecords/valid-0.tfr\n"
     ]
    }
   ],
   "source": [
    "! ls -l data/heart_l131k/tfrecords/*.tfr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
