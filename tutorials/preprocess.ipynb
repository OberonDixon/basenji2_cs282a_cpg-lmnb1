{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing a dataset for Basenji training involves a series of design choices.\n",
    "\n",
    "The input you bring to the pipeline is:\n",
    "* BigWig coverage tracks\n",
    "* Genome FASTA file\n",
    "\n",
    "First, make sure you have an hg19 FASTA file visible. If you have it already, put a symbolic link into the data directory. Otherwise, I have a machine learning friendly simplified version you can download in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, subprocess\n",
    "\n",
    "if not os.path.isfile('data/hg19.ml.fa'):\n",
    "    subprocess.call('curl -o data/hg19.ml.fa https://storage.googleapis.com/basenji_tutorial_data/hg19.ml.fa', shell=True)\n",
    "    subprocess.call('curl -o data/hg19.ml.fa.fai https://storage.googleapis.com/basenji_tutorial_data/hg19.ml.fa.fai', shell=True)                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's grab a few CAGE datasets from FANTOM5 related to heart biology.\n",
    "\n",
    "These data were processed by\n",
    "1. Aligning with Bowtie2 with very sensitive alignment parameters.\n",
    "2. Distributing multi-mapping reads and estimating genomic coverage with [bam_cov.py](https://github.com/calico/basenji/blob/master/bin/bam_cov.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.isfile('data/CNhs11760.bw'):\n",
    "    subprocess.call('curl -o data/CNhs11760.bw https://storage.googleapis.com/basenji_tutorial_data/CNhs11760.bw', shell=True)\n",
    "    subprocess.call('curl -o data/CNhs12843.bw https://storage.googleapis.com/basenji_tutorial_data/CNhs12843.bw', shell=True)\n",
    "    subprocess.call('curl -o data/CNhs12856.bw https://storage.googleapis.com/basenji_tutorial_data/CNhs12856.bw', shell=True)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we'll write out these BigWig files and labels to a samples table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "samples_out = open('data/heart_wigs.txt', 'w')\n",
    "print('aorta\\tdata/CNhs11760.bw', file=samples_out)\n",
    "print('artery\\tdata/CNhs12843.bw', file=samples_out)\n",
    "print('pulmonic_valve\\tdata/CNhs12856.bw', file=samples_out)\n",
    "samples_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to choose genomic sequences to form batches for stochastic gradient descent, divide them into training/validation/test sets, and form a single file to provide to downstream programs.\n",
    "\n",
    "The script [basenji_hdf5_single.py](https://github.com/calico/basenji/blob/master/bin/basenji_hdf5_single.py) implements this procedure.\n",
    "\n",
    "The most relevant options here are:\n",
    "\n",
    "| Option/Argument | Value | Note |\n",
    "|:---|:---|:---|\n",
    "| -d | 0.05 | Down-sample the genome to 10% to speed things up here. |\n",
    "| -g | data/unmap_macro.bed | Dodge large-scale unmappable regions like assembly gaps. |\n",
    "| -l | 262144 | Sequence length. |\n",
    "| -o | data/heart_l262k.bed | Write out the chosen sequences to a BED file. |\n",
    "| -p | 4 | Uses multiple threads with the multiprocessing library. |\n",
    "| -s | 131072 | Stride the sequences. By setting this to half the sequence length, the sequences will overlap by half. \n",
    "| -t | chr9 | Hold out chr9 sequences for testing. |\n",
    "| -w | 128 | Pools the nucleotide-resolution values to 128 bp bins. |\n",
    "| -v | chr8 | Hold out chr8 sequences for validation. |\n",
    "| fasta_file| data/hg19.fa | FASTA file to extract sequences from. |\n",
    "| sample_wigs_file | data/heart_wigs.txt | Samples and BigWig paths. |\n",
    "| hdf5_file | data/heart_l262k.h5 | Output HDF5 file. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! basenji_hdf5_single.py -d .05 -g data/unmap_macro.bed -l 262144 -o data/heart_l262k.bed -p 4 -s 131072 -t chr9 -w 128 -v chr8 data/hg19.fa data/heart_wigs.txt data/heart_l262k.h5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can offer data/heart_l262k.h5 to [basenji_train.py](https://github.com/calico/basenji/blob/master/bin/basenji_train.py) to train a model.\n",
    "\n",
    "Inside are the following data structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "ml_h5 = h5py.File('data/heart_l262k.h5')\n",
    "print(list(ml_h5.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('train_in', ml_h5['train_in'].shape)\n",
    "print('train_out', ml_h5['train_out'].shape)\n",
    "\n",
    "print('valid_in', ml_h5['valid_in'].shape)\n",
    "print('valid_out', ml_h5['valid_out'].shape)\n",
    "\n",
    "print('test_in', ml_h5['test_in'].shape)\n",
    "print('test_out', ml_h5['test_out'].shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
