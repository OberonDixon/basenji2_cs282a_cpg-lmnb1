{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train a model, you first need to convert your sequences and targets into the input HDF5 format. Check out my tutorials for how to do that; they're linked from the [main page](../README.md).\n",
    "\n",
    "For this tutorial, grab a small example HDF5 that I constructed here with 10% of the training sequences and only GM12878 targets for various DNase-seq, ChIP-seq, and CAGE experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, subprocess\n",
    "\n",
    "if not os.path.isfile('data/heart_l131k.h5'):\n",
    "    subprocess.call('curl -o heart_l131k.h5 https://storage.googleapis.com/basenji_tutorial_data/heart_l131k.h5', shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you need to decide what sort of architecture to use. This grammar probably needs work; my goal was to enable hyperparameter searches to write the parameters to file so that I could run parallel training jobs to explore the hyperparameter space. I included an example set of parameters that will work well with this data in models/params_small.txt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, run [basenji_train.py](https://github.com/calico/basenji/blob/master/bin/basenji_train.py) to train a model. The program will offer training feedback via stdout and write the model output files to the prefix given by the *-s* parameter.\n",
    "\n",
    "The most relevant options here are:\n",
    "\n",
    "| Option/Argument | Value | Note |\n",
    "|:---|:---|:---|\n",
    "| --augment_rc | | Process even-numbered epochs as forward, odd-numbered as reverse complemented. |\n",
    "| --ensemble_rc | | Average forward and reverse complemented predictions on validation set. |\n",
    "| --augment_shifts | \"1,0,-1\" | Rotate epochs over small sequence shifts. |\n",
    "| --logdir | models/heart | Directory to save training logs and model checkpoints. |\n",
    "| --params | models/params_small.txt | Table of parameters to setup the model architecture and optimization. |\n",
    "| --data | data/heart_l131k.h5 | HDF5 file containing the training and validation input and output datasets as generated by [basenji_hdf5_single.py](https://github.com/calico/basenji/blob/master/bin/basenji_hdf5_single.py) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to train, uncomment the following line and run it. Depending on your hardware, it may require several hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ! basenji_train.py --augment_rc --ensemble_rc --augment_shifts \"1,0,-1\" --logdir models/heart --params models/params_small.txt --data data/heart_l131k.h5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can just download a trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.isdir('models/heart'):\n",
    "    os.mkdir('models/heart')\n",
    "if not os.path.isfile('models/heart/model_best.tf.meta'):\n",
    "    subprocess.call('curl -o models/heart/model_best.tf.index https://storage.googleapis.com/basenji_tutorial_data/model_best.tf.index', shell=True)\n",
    "    subprocess.call('curl -o models/heart/model_best.tf.meta https://storage.googleapis.com/basenji_tutorial_data/model_best.tf.meta', shell=True)\n",
    "    subprocess.call('curl -o models/heart/model_best.tf.data-00000-of-00001 https://storage.googleapis.com/basenji_tutorial_data/model_best.tf.data-00000-of-00001', shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "models/heart/model_best.tf will now specify the name of your saved model to be provided to other programs.\n",
    "\n",
    "To further benchmark the accuracy (e.g. computing significant \"peak\" accuracy), use [basenji_test.py](https://github.com/calico/basenji/blob/master/bin/basenji_test.py).\n",
    "\n",
    "The most relevant options here are:\n",
    "\n",
    "| Option/Argument | Value | Note |\n",
    "|:---|:---|:---|\n",
    "| --rc | | Average the forward and reverse complement to form prediction. |\n",
    "| -o | output/heart_test | Output directory. |\n",
    "| --ai | 0,1,2 | Make accuracy scatter plots for targets 0, 1, and 2. |\n",
    "| --ti | 3,4,5 | Make BigWig tracks for targets 3, 4, and 5. |\n",
    "| -t | data/heart.bed | BED file describing sequence regions for BigWig track output. |\n",
    "| params_file | models/params_small.txt | Table of parameters to setup the model architecture and optimization. |\n",
    "| model_file | models/heart/model_best.tf | Trained saved model prefix. |\n",
    "| data_file | data/heart_l131k.h5 | HDF5 file containing the test input and output datasets as generated by [basenji_hdf5_single.py](https://github.com/calico/basenji/blob/master/bin/basenji_hdf5_single.py) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/davidkelley/anaconda3/lib/python3.5/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/Users/davidkelley/anaconda3/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n",
      "{'optimizer': 'adam', 'num_targets': 3, 'cnn_dense': [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0], 'adam_beta1': 0.97, 'target_pool': 128, 'cnn_dilation': [1, 1, 1, 1, 1, 2, 4, 8, 16, 32, 64, 1], 'batch_buffer': 4096, 'adam_beta2': 0.98, 'learning_rate': 0.002, 'cnn_filters': [128, 128, 192, 256, 256, 32, 32, 32, 32, 32, 32, 384], 'cnn_filter_sizes': [20, 7, 7, 7, 3, 3, 3, 3, 3, 3, 3, 1], 'cnn_dropout': 0.1, 'batch_size': 4, 'loss': 'poisson', 'link': 'softplus', 'cnn_pool': [2, 4, 4, 4, 1, 0, 0, 0, 0, 0, 0, 0]}\n",
      "Targets pooled by 128 to length 1024\n",
      "Convolution w/ 3 384x1 filters to final targets\n",
      "Model building time 11s\n",
      "2018-05-15 14:23:46.495192: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2018-05-15 14:23:50.633997: W tensorflow/core/framework/op_kernel.cc:1318] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Data loss: not an sstable (bad magic number)\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/davidkelley/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1322, in _do_call\n",
      "    return fn(*args)\n",
      "  File \"/Users/davidkelley/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1307, in _run_fn\n",
      "    options, feed_dict, fetch_list, target_list, run_metadata)\n",
      "  File \"/Users/davidkelley/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1409, in _call_tf_sessionrun\n",
      "    run_metadata)\n",
      "tensorflow.python.framework.errors_impl.DataLossError: not an sstable (bad magic number)\n",
      "\t [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/davidkelley/code/Basenji/bin/basenji_test.py\", line 702, in <module>\n",
      "    main()\n",
      "  File \"/Users/davidkelley/code/Basenji/bin/basenji_test.py\", line 253, in main\n",
      "    saver.restore(sess, model_file)\n",
      "  File \"/Users/davidkelley/anaconda3/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 1802, in restore\n",
      "    {self.saver_def.filename_tensor_name: save_path})\n",
      "  File \"/Users/davidkelley/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 900, in run\n",
      "    run_metadata_ptr)\n",
      "  File \"/Users/davidkelley/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1135, in _run\n",
      "    feed_dict_tensor, options, run_metadata)\n",
      "  File \"/Users/davidkelley/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1316, in _do_run\n",
      "    run_metadata)\n",
      "  File \"/Users/davidkelley/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1335, in _do_call\n",
      "    raise type(e)(node_def, op, message)\n",
      "tensorflow.python.framework.errors_impl.DataLossError: not an sstable (bad magic number)\n",
      "\t [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\n",
      "\n",
      "Caused by op 'save/RestoreV2', defined at:\n",
      "  File \"/Users/davidkelley/code/Basenji/bin/basenji_test.py\", line 702, in <module>\n",
      "    main()\n",
      "  File \"/Users/davidkelley/code/Basenji/bin/basenji_test.py\", line 249, in main\n",
      "    saver = tf.train.Saver()\n",
      "  File \"/Users/davidkelley/anaconda3/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 1338, in __init__\n",
      "    self.build()\n",
      "  File \"/Users/davidkelley/anaconda3/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 1347, in build\n",
      "    self._build(self._filename, build_save=True, build_restore=True)\n",
      "  File \"/Users/davidkelley/anaconda3/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 1384, in _build\n",
      "    build_save=build_save, build_restore=build_restore)\n",
      "  File \"/Users/davidkelley/anaconda3/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 835, in _build_internal\n",
      "    restore_sequentially, reshape)\n",
      "  File \"/Users/davidkelley/anaconda3/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 472, in _AddRestoreOps\n",
      "    restore_sequentially)\n",
      "  File \"/Users/davidkelley/anaconda3/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 886, in bulk_restore\n",
      "    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)\n",
      "  File \"/Users/davidkelley/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 1463, in restore_v2\n",
      "    shape_and_slices=shape_and_slices, dtypes=dtypes, name=name)\n",
      "  File \"/Users/davidkelley/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n",
      "    op_def=op_def)\n",
      "  File \"/Users/davidkelley/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 3392, in create_op\n",
      "    op_def=op_def)\n",
      "  File \"/Users/davidkelley/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1718, in __init__\n",
      "    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n",
      "\n",
      "DataLossError (see above for traceback): not an sstable (bad magic number)\n",
      "\t [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! basenji_test.py --rc -o output/heart_test --ai 0,1,2 -t data/heart.bed --ti 0,1,2 --peaks models/params_small.txt models/heart/model_best.tf data/heart_l131k.h5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*data/gm12878_test/acc.txt* is a table specifiying the loss function value, R2, R2 after log2, and Spearman correlation for each dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0  2.61342  0.06309  0.25821  0.21923  ENCSR000EJD_3_1\n",
      "   1  2.21471  0.13869  0.42051  0.31831  ENCSR000EMT_2_1\n",
      "   2  1.38732  0.14672  0.40900  0.32414  ENCSR000EMT_1_1\n",
      "   3  2.77317  0.07130  0.27155  0.24821  ENCSR000EJD_1_1\n",
      "   4  2.37138  0.09846  0.33517  0.31370  ENCSR000EJD_2_1\n",
      "   5  1.70094  0.34714  0.63973  0.43597  ENCSR057BWO_2_1\n",
      "   6  1.19770  0.08201  0.30734  0.31978  ENCSR000AKE_1_1\n",
      "   7  0.87382  0.05702  0.26991  0.29605  ENCSR000AKF_2_1\n",
      "   8  1.02482  0.22666  0.51480  0.34158  ENCSR000AOV_2_1\n",
      "   9  0.82986  0.04125  0.20892  0.23112  ENCSR000AKI_2_1\n",
      "  10  2.34713  0.37184  0.70600  0.45844  ENCSR000AKA_2_1\n",
      "  11  0.99584  0.02339  0.17299  0.20299  ENCSR000AOX_2_1\n",
      "  12  1.11231  0.12795  0.36861  0.41651  ENCSR000DRW_1_1\n",
      "  13  1.22254  0.24231  0.54471  0.50531  ENCSR000AOW_1_1\n",
      "  14  1.15009  0.02822  0.21957  0.23801  ENCSR000AKD_1_1\n",
      "  15  0.95330  0.09574  0.33044  0.35003  ENCSR000AKE_2_1\n",
      "  16  1.08106  0.13214  0.37501  0.42033  ENCSR000DRW_2_1\n",
      "  17  1.37785  0.32695  0.64537  0.42389  ENCSR000AKA_1_1\n",
      "  18  0.84082  0.37385  0.65334  0.41954  ENCSR000AKH_1_1\n",
      "  19  1.05298  0.02754  0.19704  0.20868  ENCSR000AOX_1_1\n",
      "  20  0.76286  0.25282  0.53027  0.45187  ENCSR000AKG_1_1\n",
      "  21  1.45275  0.26159  0.54841  0.37678  ENCSR057BWO_1_1\n",
      "  22  0.94717  0.40123  0.74835  0.60221  ENCSR000DRY_2_1\n",
      "  23  1.08205  0.40114  0.74568  0.59089  ENCSR000DRY_1_1\n",
      "  24  1.10889  0.12333  0.37914  0.40299  ENCSR000DRX_2_1\n",
      "  25  0.62330  0.21808  0.49323  0.43171  ENCSR000AKG_2_1\n",
      "  26  0.73553  0.05772  0.25689  0.27416  ENCSR000AKD_2_1\n",
      "  27  0.66310  0.14384  0.41311  0.37797  ENCSR000AKC_2_1\n",
      "  28  0.98232  0.06432  0.25977  0.37375  ENCSR000AOX_3_1\n",
      "  29  0.93226  0.13051  0.38197  0.40629  ENCSR000DRX_1_1\n",
      "  30  2.17985  0.03969  0.26682  0.28843  ENCSR000AKF_1_1\n",
      "  31  1.16643  0.25883  0.55751  0.52370  ENCSR000AOW_2_1\n",
      "  32  0.96793  0.05734  0.24779  0.28543  ENCSR000AKI_1_1\n",
      "  33  0.66969  0.29444  0.58879  0.37497  ENCSR000AKH_2_1\n",
      "  34  0.96620  0.11505  0.38830  0.35546  ENCSR000AKC_1_1\n",
      "  35  1.22508  0.25033  0.53892  0.35515  ENCSR000AOV_1_1\n",
      "  36  0.43857  0.22563  0.52246  0.52926  CNhs12332\n",
      "  37  0.45217  0.20710  0.50437  0.50462  CNhs12333\n",
      "  38  0.49701  0.22498  0.51689  0.51745  CNhs12331\n"
     ]
    }
   ],
   "source": [
    "! cat output/heart_test/acc.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*output/heart_test/peak.txt* is a table specifiying the number of peaks called, AUROC, and AUPRC for each dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0     627  0.62060  0.21187\n",
      "   1     194  0.73505  0.22926\n",
      "   2     124  0.80325  0.24490\n",
      "   3     867  0.65384  0.28815\n",
      "   4     644  0.68224  0.27852\n",
      "   5     267  0.77985  0.33800\n",
      "   6     343  0.73019  0.17062\n",
      "   7     191  0.75358  0.11675\n",
      "   8     143  0.76431  0.28520\n",
      "   9       3  0.68574  0.00182\n",
      "  10     350  0.77412  0.37582\n",
      "  11     184  0.60722  0.05175\n",
      "  12     295  0.78291  0.19798\n",
      "  13     324  0.88868  0.40744\n",
      "  14     130  0.64108  0.06255\n",
      "  15     289  0.77005  0.14116\n",
      "  16     273  0.78886  0.18852\n",
      "  17     201  0.82412  0.37455\n",
      "  18     116  0.82394  0.39929\n",
      "  19      98  0.64931  0.02932\n",
      "  20     189  0.84032  0.33949\n",
      "  21     108  0.84197  0.43734\n",
      "  22      95  0.96172  0.57619\n",
      "  23     104  0.95983  0.58563\n",
      "  24     145  0.73452  0.07188\n",
      "  25     182  0.81640  0.30829\n",
      "  26      55  0.65197  0.03704\n",
      "  27      94  0.82064  0.30024\n",
      "  28     202  0.72931  0.08968\n",
      "  29     117  0.73809  0.05621\n",
      "  30     468  0.70399  0.22005\n",
      "  31     318  0.88671  0.39525\n",
      "  32      27  0.66270  0.00940\n",
      "  33      63  0.90426  0.49826\n",
      "  34     122  0.83192  0.32991\n",
      "  35     177  0.76714  0.28138\n",
      "  36      70  0.89609  0.25780\n",
      "  37      76  0.85589  0.19604\n",
      "  38      75  0.85829  0.23309\n"
     ]
    }
   ],
   "source": [
    "! cat data/heart_test/peaks.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The directories *pr*, *roc*, *violin*, and *scatter* in *data/heart_test* contain plots for the targets indexed by 0, 1, and 2 as specified by the --ai option above.\n",
    "\n",
    "E.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"600\"\n",
       "            height=\"500\"\n",
       "            src=\"data/gm12878_test/pr/t0.pdf\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1088a7a90>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame('output/heart_test/pr/t0.pdf', width=600, height=500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
