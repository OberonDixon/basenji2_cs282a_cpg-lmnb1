{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train a model, you first need to convert your sequences and targets into the input HDF5 format. Check out my tutorials for how to do that; they're linked from the [main page](../README.md).\n",
    "\n",
    "For this tutorial, grab a small example HDF5 that I constructed here with 10% of the training sequences and only GM12878 targets for various DNase-seq, ChIP-seq, and CAGE experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, subprocess\n",
    "\n",
    "if not os.path.isfile('data/gm12878_l262k_w128_d10.h5'):\n",
    "    subprocess.call('curl -o data/gm12878_l262k_w128_d10.h5 https://storage.googleapis.com/262k_binned/gm12878_l262k_w128_d10.h5', shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you need to decide what sort of architecture to use. This grammar probably needs work; my goal was to enable hyperparameter searches to write the parameters to file so that I could run parallel training jobs to explore the hyperparameter space. I included an example set of parameters that will work well with this data in models/params_small.txt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, run [basenji_train.py](https://github.com/calico/basenji/blob/master/bin/basenji_train.py) to train a model. The program will offer training feedback via stdout and write the model output files to the prefix given by the *-s* parameter.\n",
    "\n",
    "The most relevant options here are:\n",
    "\n",
    "| Option/Argument | Value | Note |\n",
    "|:---|:---|:---|\n",
    "| --rc | | Process even-numbered epochs as forward, odd-numbered as reverse complemented. Average the forward and reverse complement to assess validation accuracy. |\n",
    "| -s | models/gm12878 | File path prefix to save the model. |\n",
    "| params_file | models/params_small.txt | Table of parameters to setup the model architecture and optimization. |\n",
    "| data_file | data/gm12878_l262k_w128_d10.h5 | HDF5 file containing the training and validation input and output datasets as generated by [basenji_hdf5_single.py](https://github.com/calico/basenji/blob/master/bin/basenji_hdf5_single.py) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to train, uncomment the following line and run it. Depending on your hardware, it may require many hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/davidkelley/anaconda3/lib/python3.5/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/Users/davidkelley/anaconda3/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n",
      "{'optimizer': 'adam', 'cnn_filters': [128, 160, 200, 250, 256, 32, 32, 32, 32, 32, 32, 384], 'loss': 'poisson', 'cnn_dropout': 0.05, 'learning_rate': 0.002, 'batch_buffer': 16384, 'cnn_filter_sizes': [20, 6, 6, 6, 3, 3, 3, 3, 3, 3, 3, 1], 'batch_size': 1, 'adam_beta2': 0.98, 'batch_renorm': 1, 'link': 'softplus', 'cnn_dense': [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0], 'adam_beta1': 0.97, 'cnn_dilation': [1, 1, 1, 1, 1, 2, 4, 8, 16, 32, 64, 1], 'cnn_pool': [2, 4, 4, 4, 1, 0, 0, 0, 0, 0, 0, 0]}\n",
      "Targets pooled by 128 to length 2048\n",
      "Convolution w/ 39 384x1 filters to final targets\n",
      "Model building time 11.449232\n",
      "Batcher initialized\n",
      "2018-05-11 16:23:40.480890: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.2 AVX AVX2 FMA\n",
      "Initializing...\n",
      "Initialization time 17.491073\n"
     ]
    }
   ],
   "source": [
    "! basenji_train.py --logdir models/gm12878 --params models/params_small.txt --data data/gm12878_l262k_w128_d10.h5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can just download a trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.isfile('models/gm12878_d10/model_best.tf.meta'):\n",
    "    subprocess.call('curl -o models/gm12878_d10.tf.index https://storage.googleapis.com/basenji_tutorial_data/model_gm12878_d10.tf.index', shell=True)\n",
    "    subprocess.call('curl -o models/gm12878_d10.tf.meta https://storage.googleapis.com/basenji_tutorial_data/model_gm12878_d10.tf.meta', shell=True)\n",
    "    subprocess.call('curl -o models/gm12878_d10.tf.data-00000-of-00001 https://storage.googleapis.com/basenji_tutorial_data/model_gm12878_d10.tf.data-00000-of-00001', shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "models/gm12878_best.tf will now specify the name of your saved model to be provided to other programs.\n",
    "\n",
    "To further benchmark the accuracy (e.g. computing significant \"peak\" accuracy), use [basenji_test.py](https://github.com/calico/basenji/blob/master/bin/basenji_test.py).\n",
    "\n",
    "The most relevant options here are:\n",
    "\n",
    "| Option/Argument | Value | Note |\n",
    "|:---|:---|:---|\n",
    "| --rc | | Average the forward and reverse complement to form prediction. |\n",
    "| -o | data/gm12878_test | Output directory. |\n",
    "| --ai | 0,1,2 | Make accuracy scatter plots for targets 0, 1, and 2. |\n",
    "| --ti | 3,4,5 | Make BigWig tracks for targets 3, 4, and 5. |\n",
    "| -t | data/gm12878_l262k_w128_d10.bed | BED file describing sequence regions for BigWig track output. |\n",
    "| params_file | models/params_small.txt | Table of parameters to setup the model architecture and optimization. |\n",
    "| model_file | models/gm12878_d10.tf | Trained saved model prefix. |\n",
    "| data_file | data/gm12878_l262k_w128_d10.h5 | HDF5 file containing the test input and output datasets as generated by [basenji_hdf5_single.py](https://github.com/calico/basenji/blob/master/bin/basenji_hdf5_single.py) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_buffer': 16384, 'loss': 'poisson', 'full_dropout': 0.05, 'adam_beta1': 0.97, 'cnn_filter_sizes': [22, 1, 6, 6, 6, 3], 'cnn_filters': [128, 128, 160, 200, 250, 256], 'adam_beta2': 0.98, 'dcnn_filter_sizes': [3, 3, 3, 3, 3, 3], 'dense': 1, 'full_units': 384, 'learning_rate': 0.002, 'link': 'softplus', 'cnn_dropout': 0.05, 'cnn_pool': [1, 2, 4, 4, 4, 1], 'batch_size': 1, 'batch_renorm': 1, 'dcnn_dropout': 0.1, 'dcnn_filters': [32, 32, 32, 32, 32, 32]}\n",
      "Targets pooled by 128 to length 2048\n",
      "Convolution w/ 128 4x22 filters strided by 1\n",
      "Batch normalization\n",
      "ReLU\n",
      "Dropout w/ probability 0.050\n",
      "Convolution w/ 128 128x1 filters strided by 1\n",
      "Batch normalization\n",
      "ReLU\n",
      "Max pool 2\n",
      "Dropout w/ probability 0.050\n",
      "Convolution w/ 160 128x6 filters strided by 1\n",
      "Batch normalization\n",
      "ReLU\n",
      "Max pool 4\n",
      "Dropout w/ probability 0.050\n",
      "Convolution w/ 200 160x6 filters strided by 1\n",
      "Batch normalization\n",
      "ReLU\n",
      "Max pool 4\n",
      "Dropout w/ probability 0.050\n",
      "Convolution w/ 250 200x6 filters strided by 1\n",
      "Batch normalization\n",
      "ReLU\n",
      "Max pool 4\n",
      "Dropout w/ probability 0.050\n",
      "Convolution w/ 256 250x3 filters strided by 1\n",
      "Batch normalization\n",
      "ReLU\n",
      "Dropout w/ probability 0.050\n",
      "Dilated convolution w/ 32 256x3 rate 2 filters\n",
      "Batch normalization\n",
      "ReLU\n",
      "Dropout w/ probability 0.100\n",
      "Dilated convolution w/ 32 288x3 rate 4 filters\n",
      "Batch normalization\n",
      "ReLU\n",
      "Dropout w/ probability 0.100\n",
      "Dilated convolution w/ 32 320x3 rate 8 filters\n",
      "Batch normalization\n",
      "ReLU\n",
      "Dropout w/ probability 0.100\n",
      "Dilated convolution w/ 32 352x3 rate 16 filters\n",
      "Batch normalization\n",
      "ReLU\n",
      "Dropout w/ probability 0.100\n",
      "Dilated convolution w/ 32 384x3 rate 32 filters\n",
      "Batch normalization\n",
      "ReLU\n",
      "Dropout w/ probability 0.100\n",
      "Dilated convolution w/ 32 416x3 rate 64 filters\n",
      "Batch normalization\n",
      "ReLU\n",
      "Dropout w/ probability 0.100\n",
      "Linear transformation 448x384\n",
      "Batch normalization\n",
      "ReLU\n",
      "Dropout w/ probability 0.050\n",
      "Linear transform 384x39x1\n",
      "Model building time 8s\n",
      "2017-08-25 13:44:24.196765: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2017-08-25 13:44:24.196787: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2017-08-25 13:44:24.196803: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2017-08-25 13:44:24.196807: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\n",
      "SeqNN test: 154s\n",
      "Test Loss:      1.17219\n",
      "Test R2:        0.23081\n",
      "Test log R2:    0.15924\n",
      "Test SpearmanR: 0.29397\n",
      "Test AUROC:     0.79533\n",
      "Test AUPRC:     0.25234\n"
     ]
    }
   ],
   "source": [
    "! basenji_test.py --rc -o data/gm12878_test --ai 0,1,2 -t data/gm12878_l262k_w128_d10.bed --ti 3,4,5 models/params_small.txt models/gm12878_d10/model_best.tf data/gm12878_l262k_w128_d10.h5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*data/gm12878_test/acc.txt* is a table specifiying the loss function value, R2, R2 after log2, and Spearman correlation for each dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0  2.55459  0.12923  0.06884  0.20332  ENCSR000EJD_3_1\n",
      "   1  2.07825  0.26504  0.11945  0.25228  ENCSR000EMT_2_1\n",
      "   2  1.34195  0.23470  0.12942  0.26919  ENCSR000EMT_1_1\n",
      "   3  2.73190  0.11670  0.08710  0.26056  ENCSR000EJD_1_1\n",
      "   4  2.34003  0.13552  0.10720  0.28923  ENCSR000EJD_2_1\n",
      "   5  1.65317  0.42328  0.22528  0.37787  ENCSR057BWO_2_1\n",
      "   6  1.15429  0.17104  0.17254  0.34555  ENCSR000AKE_1_1\n",
      "   7  0.84690  0.08580  0.07383  0.30523  ENCSR000AKF_2_1\n",
      "   8  1.01278  0.30182  0.09221  0.16016  ENCSR000AOV_2_1\n",
      "   9  0.82520  0.05287  0.06443  0.26996  ENCSR000AKI_2_1\n",
      "  10  2.23546  0.50709  0.20589  0.29792  ENCSR000AKA_2_1\n",
      "  11  1.02572  0.03412  0.03277  0.14032  ENCSR000AOX_2_1\n",
      "  12  1.06876  0.19844  0.23515  0.46800  ENCSR000DRW_1_1\n",
      "  13  1.23316  0.21266  0.22506  0.38231  ENCSR000AOW_1_1\n",
      "  14  1.13732  0.06730  0.08054  0.29732  ENCSR000AKD_1_1\n",
      "  15  0.91039  0.18123  0.20173  0.38487  ENCSR000AKE_2_1\n",
      "  16  1.04031  0.20278  0.23683  0.47334  ENCSR000DRW_2_1\n",
      "  17  1.33496  0.44043  0.18933  0.25987  ENCSR000AKA_1_1\n",
      "  18  0.81793  0.40859  0.17472  0.22379  ENCSR000AKH_1_1\n",
      "  19  1.07089  0.04304  0.04167  0.17603  ENCSR000AOX_1_1\n",
      "  20  0.73753  0.28146  0.17093  0.26443  ENCSR000AKG_1_1\n",
      "  21  1.44393  0.30865  0.14499  0.28138  ENCSR057BWO_1_1\n",
      "  22  0.88583  0.56802  0.33412  0.30759  ENCSR000DRY_2_1\n",
      "  23  1.00782  0.56704  0.33325  0.30656  ENCSR000DRY_1_1\n",
      "  24  1.11359  0.13386  0.16526  0.43256  ENCSR000DRX_2_1\n",
      "  25  0.60160  0.24526  0.15597  0.24771  ENCSR000AKG_2_1\n",
      "  26  0.73290  0.06981  0.08439  0.31987  ENCSR000AKD_2_1\n",
      "  27  0.59653  0.19110  0.14796  0.23150  ENCSR000AKC_2_1\n",
      "  28  0.99910  0.09160  0.13582  0.39286  ENCSR000AOX_3_1\n",
      "  29  0.93687  0.13457  0.16608  0.43743  ENCSR000DRX_1_1\n",
      "  30  2.10308  0.07517  0.05362  0.25481  ENCSR000AKF_1_1\n",
      "  31  1.18113  0.22252  0.23676  0.39986  ENCSR000AOW_2_1\n",
      "  32  0.95693  0.08744  0.10066  0.34157  ENCSR000AKI_1_1\n",
      "  33  0.65791  0.34125  0.13870  0.14816  ENCSR000AKH_2_1\n",
      "  34  0.86210  0.16323  0.12289  0.22392  ENCSR000AKC_1_1\n",
      "  35  1.21009  0.32089  0.11304  0.21593  ENCSR000AOV_1_1\n",
      "  36  0.40197  0.33684  0.29534  0.26826  CNhs12332\n",
      "  37  0.41519  0.31854  0.26024  0.27175  CNhs12333\n",
      "  38  0.45739  0.33262  0.28639  0.28167  CNhs12331\n"
     ]
    }
   ],
   "source": [
    "! cat data/gm12878_test/acc.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*data/gm12878_test/peak.txt* is a table specifiying the number of peaks called, AUROC, and AUPRC for each dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0     627  0.64745  0.23860\n",
      "   1     194  0.76438  0.26973\n",
      "   2     124  0.82391  0.26973\n",
      "   3     867  0.65559  0.28928\n",
      "   4     644  0.66481  0.26887\n",
      "   5     267  0.79283  0.33708\n",
      "   6     343  0.82138  0.27525\n",
      "   7     191  0.78184  0.12368\n",
      "   8     143  0.80043  0.25877\n",
      "   9       3  0.63227  0.00096\n",
      "  10     350  0.79971  0.37140\n",
      "  11     184  0.63327  0.08505\n",
      "  12     295  0.85445  0.29855\n",
      "  13     324  0.87453  0.34045\n",
      "  14     130  0.67821  0.05758\n",
      "  15     289  0.85102  0.27473\n",
      "  16     273  0.85927  0.31028\n",
      "  17     201  0.83182  0.37427\n",
      "  18     116  0.84729  0.35814\n",
      "  19      98  0.68407  0.03596\n",
      "  20     189  0.86034  0.34349\n",
      "  21     108  0.83877  0.40408\n",
      "  22      95  0.93912  0.53441\n",
      "  23     104  0.95152  0.54211\n",
      "  24     145  0.71995  0.06288\n",
      "  25     182  0.84500  0.29557\n",
      "  26      55  0.66193  0.02123\n",
      "  27      94  0.87050  0.26843\n",
      "  28     202  0.76177  0.15448\n",
      "  29     117  0.73136  0.05396\n",
      "  30     468  0.73932  0.21746\n",
      "  31     318  0.86267  0.32694\n",
      "  32      27  0.73969  0.02533\n",
      "  33      63  0.89831  0.45609\n",
      "  34     122  0.85890  0.30367\n",
      "  35     177  0.77894  0.25783\n",
      "  36      70  0.90578  0.25993\n",
      "  37      76  0.88126  0.21710\n",
      "  38      75  0.87420  0.25800\n"
     ]
    }
   ],
   "source": [
    "! cat data/gm12878_test/peaks.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The directories *pr*, *roc*, *violin*, and *scatter* in *data/gm12878_test* contain plots for the targets indexed by 0, 1, and 2 as specified by the --ai option above.\n",
    "\n",
    "E.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"600\"\n",
       "            height=\"500\"\n",
       "            src=\"data/gm12878_test/pr/t0.pdf\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x106d06e48>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame('data/gm12878_test/pr/t0.pdf', width=600, height=500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
