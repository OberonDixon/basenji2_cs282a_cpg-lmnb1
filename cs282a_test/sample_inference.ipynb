{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Inference on Different Fine-Tuning Models\n",
    "\n",
    "This notebook is intended for CS 182/282A project reviewers to verify that the models run. To re-run experiments to see how the models were trained, please take a look at the subdirectories after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import torch\n",
    "from torch import nn\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0. Test Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = f'./sample_data/test_chunk_X1.h5'\n",
    "f = h5py.File(test_path, 'r')\n",
    "dset = f['embeddings']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Linear Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearTransform(nn.Module):\n",
    "    \"\"\"Takes in input (B, 1536, 896) and outputs predictions (B, 18, 896).\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv_layer = nn.Conv1d(in_channels=1536, out_channels=18, kernel_size=1)\n",
    "        nn.init.kaiming_normal_(self.conv_layer.weight, nonlinearity='relu')\n",
    "        nn.init.zeros_(self.conv_layer.bias)\n",
    "        self.activation = nn.Softplus()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = None\n",
    "        out = self.activation(self.conv_layer(x))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearTransform(\n",
       "  (conv_layer): Conv1d(1536, 18, kernel_size=(1,), stride=(1,))\n",
       "  (activation): Softplus(beta=1, threshold=20)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_probe = LinearTransform()\n",
    "trained_probe.load_state_dict(torch.load('../cs282a_linear-probing/first_full_run.pth', map_location=torch.device('cpu')))\n",
    "trained_probe.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[185.4840, 191.4664, 192.6241,  ..., 151.3887, 155.1280, 152.4800],\n",
      "        [161.7277, 163.1386, 169.3251,  ..., 104.0094, 109.2395, 103.1794],\n",
      "        [164.3876, 167.4644, 170.2731,  ..., 130.6068, 136.7782, 134.1537],\n",
      "        ...,\n",
      "        [  5.4140,   5.1493,   7.5385,  ...,   4.1214,   4.5688,   6.1868],\n",
      "        [ 21.0667,  32.2378,  33.6077,  ...,  15.7496,  31.8367,  35.1272],\n",
      "        [ 10.4077,  13.6044,  13.3284,  ...,   6.8675,  11.5857,  14.0709]],\n",
      "       grad_fn=<SoftplusBackward0>)\n",
      "tensor([[149.2984, 153.1806, 147.6395,  ..., 161.8470, 160.1565, 155.4014],\n",
      "        [167.3268, 170.5730, 169.7932,  ..., 183.3823, 172.8373, 173.1581],\n",
      "        [163.4417, 167.6719, 162.6482,  ..., 163.1466, 160.1153, 159.9908],\n",
      "        ...,\n",
      "        [  8.0236,   8.1998,   8.9862,  ...,   6.2350,   2.3821,   4.0134],\n",
      "        [ 21.2210,  16.4974,  23.3127,  ...,  24.2986,   2.7081,  23.4745],\n",
      "        [  9.4221,   6.9032,   8.4551,  ...,  10.5540,   6.5561,  13.4993]],\n",
      "       grad_fn=<SoftplusBackward0>)\n",
      "tensor([[120.8197, 123.1387, 126.1641,  ..., 127.0645, 130.0960, 130.1491],\n",
      "        [146.2713, 141.2569, 145.8799,  ..., 150.0836, 154.9389, 154.4115],\n",
      "        [136.7463, 136.2886, 141.4921,  ..., 139.3570, 140.6851, 140.9145],\n",
      "        ...,\n",
      "        [  4.7012,  10.2509,  13.0227,  ...,   1.9024,   3.7089,   3.4383],\n",
      "        [ 48.6627,  85.9316,  88.2326,  ...,  12.0444,  11.9547,  21.7704],\n",
      "        [ 10.2859,  20.4306,  25.8561,  ...,   1.5197,   1.3879,   1.8796]],\n",
      "       grad_fn=<SoftplusBackward0>)\n",
      "tensor([[100.8116,  98.4364, 105.6130,  ...,  70.1928,  72.7492,  73.1145],\n",
      "        [ 90.0530,  89.9671,  98.5574,  ...,  72.6990,  78.8294,  77.3891],\n",
      "        [ 90.0806,  93.7803,  98.7273,  ...,  80.5035,  82.4950,  83.2536],\n",
      "        ...,\n",
      "        [ 15.9081,  18.4429,  16.2002,  ...,  25.1864,  23.0018,  24.3828],\n",
      "        [ 94.0537, 103.9371,  79.0741,  ..., 136.6346, 111.6565, 100.1512],\n",
      "        [ 18.8396,  23.7084,  16.2532,  ...,  33.6626,  29.0348,  28.1579]],\n",
      "       grad_fn=<SoftplusBackward0>)\n",
      "tensor([[ 86.4153,  91.5404,  91.4072,  ...,  82.5384,  88.6827,  91.5116],\n",
      "        [ 79.4135,  88.4076,  87.6532,  ...,  87.0676,  86.8411,  87.0274],\n",
      "        [ 93.2469, 100.4391, 104.4680,  ...,  94.8033,  98.4222, 102.8869],\n",
      "        ...,\n",
      "        [ 13.2580,  12.9110,  15.1440,  ...,  16.4358,  18.8632,  26.4835],\n",
      "        [ 93.0443,  91.1949, 101.4605,  ...,  83.9203, 108.7101, 132.0352],\n",
      "        [ 12.0665,   8.9235,  12.2126,  ...,  25.4150,  31.5540,  44.8951]],\n",
      "       grad_fn=<SoftplusBackward0>)\n",
      "tensor([[95.1763, 94.0647, 90.4310,  ..., 81.3349, 78.1558, 83.8899],\n",
      "        [92.2096, 88.1313, 87.2131,  ..., 76.4903, 78.3972, 82.9638],\n",
      "        [95.8457, 94.7216, 95.8633,  ..., 96.8638, 96.4910, 99.7771],\n",
      "        ...,\n",
      "        [10.9105, 15.5763,  9.5498,  ..., 44.8753, 22.5085, 20.0600],\n",
      "        [59.3324, 78.7221, 33.5512,  ..., 77.9804, 65.0374, 62.2979],\n",
      "        [11.4042, 19.6713,  8.0084,  ..., 57.0792, 42.4545, 38.5559]],\n",
      "       grad_fn=<SoftplusBackward0>)\n",
      "tensor([[160.9746, 160.0829, 160.6598,  ..., 181.6986, 195.7034, 194.6177],\n",
      "        [167.6630, 160.0454, 153.9482,  ..., 189.8278, 199.7147, 196.7443],\n",
      "        [183.9344, 177.7832, 179.2694,  ..., 185.6347, 195.0712, 192.9830],\n",
      "        ...,\n",
      "        [ 13.2508,   7.4765,   6.3877,  ...,   6.2613,   8.5127,  10.0019],\n",
      "        [ 99.3792,  69.3431,  54.9305,  ...,  19.0672,  24.7440,  20.3776],\n",
      "        [ 23.0416,  14.9186,  10.8556,  ...,   8.2470,   7.5086,   6.7360]],\n",
      "       grad_fn=<SoftplusBackward0>)\n",
      "tensor([[135.1252, 137.0820, 140.1319,  ..., 190.4521, 184.7673, 175.8489],\n",
      "        [156.5947, 155.8044, 158.5527,  ..., 164.3048, 159.5437, 152.7644],\n",
      "        [141.1341, 141.7263, 144.3717,  ..., 194.2375, 188.1880, 177.7010],\n",
      "        ...,\n",
      "        [  7.2904,   7.2503,   7.8259,  ...,   5.6125,   1.6961,   0.6262],\n",
      "        [ 16.3991,  17.5997,  20.4587,  ...,  46.9552,  35.1460,  33.3025],\n",
      "        [  5.0116,   5.1909,   6.1729,  ...,  13.3077,   9.1349,   9.2563]],\n",
      "       grad_fn=<SoftplusBackward0>)\n",
      "tensor([[143.9242, 143.7551, 141.8314,  ..., 115.1826, 113.4652, 112.2225],\n",
      "        [114.2138, 117.7131, 116.0454,  ..., 112.4954, 111.7554, 109.6283],\n",
      "        [121.8019, 120.1443, 121.2454,  ..., 115.8774, 115.6160, 111.3573],\n",
      "        ...,\n",
      "        [  2.5956,   0.9622,   5.9651,  ...,  17.9699,  18.9164,  16.4963],\n",
      "        [ 24.5584,  21.8136,  41.0147,  ...,  90.4778,  84.8802,  69.6820],\n",
      "        [  7.9608,   5.6379,  13.2608,  ...,  36.7875,  37.9844,  33.5178]],\n",
      "       grad_fn=<SoftplusBackward0>)\n",
      "tensor([[185.6256, 182.1289, 182.6872,  ..., 191.3248, 193.2050, 186.1759],\n",
      "        [183.9882, 184.2827, 177.4371,  ..., 202.1972, 202.4642, 196.5477],\n",
      "        [174.1942, 171.5017, 171.2132,  ..., 182.9038, 184.1622, 179.8323],\n",
      "        ...,\n",
      "        [  1.7463,   9.0015,  20.5801,  ...,   3.1847,   2.5343,   3.4549],\n",
      "        [  3.1974,  39.1155,  38.2550,  ...,  12.2985,  14.7351,  24.5370],\n",
      "        [  6.0614,  19.0701,  30.3728,  ...,   7.8549,   9.0848,  11.7355]],\n",
      "       grad_fn=<SoftplusBackward0>)\n",
      "tensor([[136.6294, 135.8548, 129.5853,  ..., 181.1891, 172.7730, 173.7408],\n",
      "        [146.4553, 150.8674, 144.2109,  ..., 174.0476, 167.3086, 171.6453],\n",
      "        [138.6785, 136.6969, 134.1494,  ..., 161.9776, 154.7500, 154.2588],\n",
      "        ...,\n",
      "        [  3.0597,   2.9064,   1.0467,  ...,   3.3320,   1.1611,   1.4181],\n",
      "        [ 17.3120,  19.5566,  13.3104,  ...,   6.0573,   5.4764,  12.1917],\n",
      "        [  6.5119,   6.2717,   4.2419,  ...,   4.2332,   4.3855,   5.1335]],\n",
      "       grad_fn=<SoftplusBackward0>)\n",
      "tensor([[176.0033, 176.9435, 174.6412,  ..., 187.3951, 189.0268, 194.0254],\n",
      "        [182.0115, 187.0910, 177.0773,  ..., 172.8142, 174.9541, 178.9300],\n",
      "        [156.3321, 155.9854, 152.8830,  ..., 167.6589, 169.9520, 174.4812],\n",
      "        ...,\n",
      "        [  1.0682,   0.7404,   2.1934,  ...,   3.2725,   4.2622,   1.9217],\n",
      "        [ 17.7647,  22.4858,  37.3967,  ...,  29.1791,  35.7003,  28.0401],\n",
      "        [ 11.4928,   9.7706,  12.3369,  ...,  13.1100,  16.1508,  11.2966]],\n",
      "       grad_fn=<SoftplusBackward0>)\n",
      "tensor([[194.1808, 202.5147, 205.6476,  ..., 203.6097, 206.6658, 203.7510],\n",
      "        [191.9242, 197.6113, 197.9460,  ..., 215.8603, 220.5305, 219.6940],\n",
      "        [185.1368, 189.0736, 191.6665,  ..., 191.9217, 196.8814, 195.3182],\n",
      "        ...,\n",
      "        [  3.7183,   4.4191,   4.9270,  ...,   3.0797,   1.4238,   0.5711],\n",
      "        [ 28.9809,  31.5952,  31.4661,  ...,  20.4664,  14.4561,   3.9761],\n",
      "        [ 24.6486,  17.1652,  11.4356,  ...,  10.2961,   9.4126,   6.8927]],\n",
      "       grad_fn=<SoftplusBackward0>)\n",
      "tensor([[210.6259, 210.0385, 208.8145,  ..., 210.2667, 214.7433, 207.9783],\n",
      "        [207.1029, 204.0776, 205.0182,  ..., 195.1041, 199.3316, 193.2540],\n",
      "        [190.0446, 188.7508, 186.9702,  ..., 194.2193, 194.2372, 193.1808],\n",
      "        ...,\n",
      "        [  6.4718,   3.9164,   4.0021,  ...,   3.6287,   5.2942,   5.9848],\n",
      "        [ 20.2461,   1.9711,   5.4015,  ...,  14.0057,  41.9240,  38.8822],\n",
      "        [ 11.1230,   4.5649,   4.8700,  ...,   5.4452,  13.2753,   9.8852]],\n",
      "       grad_fn=<SoftplusBackward0>)\n",
      "tensor([[103.7457, 106.9671, 103.7989,  ...,  76.6075,  73.4294,  77.3474],\n",
      "        [ 98.8727, 102.7212,  95.7450,  ...,  81.6389,  81.2803,  77.1050],\n",
      "        [114.5410, 119.7391, 115.8308,  ...,  82.1918,  76.5755,  76.6090],\n",
      "        ...,\n",
      "        [ 10.6931,  14.0801,  12.8923,  ...,  28.0397,  21.7476,  30.3241],\n",
      "        [ 97.5001, 105.4857, 104.4188,  ..., 122.8987,  98.3214, 128.3224],\n",
      "        [ 14.7479,  14.8297,  15.7180,  ...,  36.4556,  25.6846,  40.8785]],\n",
      "       grad_fn=<SoftplusBackward0>)\n",
      "tensor([[ 91.5988,  88.6578,  91.2273,  ...,  84.8910,  81.9346,  87.2795],\n",
      "        [ 84.8201,  81.7445,  83.9750,  ...,  86.5843,  83.2693,  86.3647],\n",
      "        [100.2505,  96.6219,  99.1552,  ...,  98.0533,  94.8482,  95.8824],\n",
      "        ...,\n",
      "        [  8.9786,   9.0102,   7.8557,  ...,  11.9987,  11.4539,  13.1633],\n",
      "        [ 50.2174,  52.1963,  45.0195,  ...,  51.2622,  61.7287,  69.8892],\n",
      "        [ 14.8969,  17.6172,  13.4850,  ...,  16.2387,  19.6914,  24.4033]],\n",
      "       grad_fn=<SoftplusBackward0>)\n",
      "tensor([[136.9517, 133.8692, 130.5684,  ..., 121.6477, 121.9856, 122.5980],\n",
      "        [118.5258, 119.1527, 117.2553,  ..., 140.9175, 139.8342, 143.4988],\n",
      "        [155.1589, 156.0416, 151.2865,  ..., 137.2667, 138.4543, 139.5754],\n",
      "        ...,\n",
      "        [  5.4858,   8.9273,   6.5205,  ...,  21.7881,  13.7627,   5.4378],\n",
      "        [ 40.5386,  45.9276,  33.2505,  ..., 108.6880,  82.6607,  35.4739],\n",
      "        [ 10.8136,  14.8749,   9.2885,  ...,  39.0681,  20.5710,   6.7025]],\n",
      "       grad_fn=<SoftplusBackward0>)\n",
      "tensor([[105.8478, 102.5805, 100.0712,  ..., 131.7265, 131.5739, 129.5936],\n",
      "        [106.8062, 102.1125, 105.4622,  ..., 117.8459, 122.9937, 117.6972],\n",
      "        [122.1957, 119.2631, 116.2481,  ..., 148.8608, 152.8770, 152.4770],\n",
      "        ...,\n",
      "        [  6.8704,   7.1327,  12.7816,  ...,   9.8082,  11.4898,  10.8973],\n",
      "        [ 61.2758,  72.1420, 128.8212,  ...,  54.6988,  55.9743,  48.1546],\n",
      "        [  3.4916,   5.3792,  17.6700,  ...,   8.4775,   9.6041,   8.0654]],\n",
      "       grad_fn=<SoftplusBackward0>)\n",
      "tensor([[158.2212, 154.3656, 150.4650,  ..., 148.1606, 146.3219, 142.1782],\n",
      "        [123.5908, 119.1272, 125.0653,  ..., 139.0609, 135.8995, 133.9052],\n",
      "        [166.1274, 160.6514, 162.2100,  ..., 157.9064, 153.8508, 152.9004],\n",
      "        ...,\n",
      "        [  7.6560,   4.9750,   7.4695,  ...,   8.4624,   8.4595,   8.0842],\n",
      "        [ 53.1712,  24.9585,  23.2711,  ...,  44.3170,  42.2589,  45.5707],\n",
      "        [ 19.0669,   9.3996,   9.3469,  ...,  17.6809,  17.6098,  19.2366]],\n",
      "       grad_fn=<SoftplusBackward0>)\n",
      "tensor([[126.5435, 123.8108, 120.9309,  ..., 103.8299,  99.9147,  99.5980],\n",
      "        [126.0537, 121.5540, 123.3926,  ...,  91.3941,  95.0367,  90.3676],\n",
      "        [144.4781, 142.1303, 138.9446,  ..., 112.9538, 110.3071, 109.6088],\n",
      "        ...,\n",
      "        [ 11.6682,  13.2369,   7.3810,  ...,   7.7217,  16.9870,  29.8214],\n",
      "        [112.2037, 122.5116,  82.9366,  ...,  59.8412, 112.5167, 147.7665],\n",
      "        [ 15.6564,  18.9398,   9.1556,  ...,  11.2784,  31.2660,  50.1304]],\n",
      "       grad_fn=<SoftplusBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(dset)):\n",
    "    inputs = torch.Tensor(dset[i])\n",
    "    predictions = trained_probe(inputs.transpose(0,1))\n",
    "    print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 1D CNN + Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPModel(nn.Module):\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super(MLPModel, self).__init__(*args, **kwargs)\n",
    "\n",
    "        self.layers = nn.Sequential(OrderedDict([\n",
    "            ('conv1x1', nn.Conv1d(1536, 500, 1)),\n",
    "            ('gelu1', nn.GELU()),\n",
    "            ('flatten', nn.Flatten()),\n",
    "            ('fc1', nn.Linear(448000, 18))\n",
    "        ]))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPModel(\n",
       "  (layers): Sequential(\n",
       "    (conv1x1): Conv1d(1536, 500, kernel_size=(1,), stride=(1,))\n",
       "    (gelu1): GELU(approximate='none')\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "    (fc1): Linear(in_features=448000, out_features=18, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_model = MLPModel()\n",
    "mlp_model.load_state_dict(torch.load('../cs282a_conv1d_perceptron/model_20231128_063541_2'))\n",
    "mlp_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[185.1219, 165.0662, 172.5271, 169.7588, 183.6932, 167.8994, 186.5149,\n",
      "         182.3538, 168.5229, 176.6938, 176.6521, 160.2308,  13.9360,   9.6113,\n",
      "          67.5173,   3.1071,  77.7010,   9.5490]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[150.9066, 182.2299, 153.4019, 159.0182, 148.4989, 159.1315, 152.8746,\n",
      "         141.5418, 171.3732, 160.7533, 153.8532, 165.8867,  14.1168,   9.8270,\n",
      "         125.0091,   3.1626,  92.7511,   8.0396]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[130.3526, 155.1663, 149.7936, 150.6678, 148.9104, 131.7747, 154.3013,\n",
      "         136.7864, 141.5564, 164.5454, 136.9123, 130.3914,   9.1082,   4.9956,\n",
      "         150.3599,   3.4720, 143.1677,   9.8213]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[102.7417, 111.9067, 121.7186, 124.8033, 115.8242, 115.5144, 118.2789,\n",
      "         117.0127, 105.2756, 111.5346, 111.8413, 114.4374,   2.5230,   3.3681,\n",
      "         207.3947,   9.7393, 218.1850,  23.6601]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 84.6395,  82.0686,  96.8328, 113.9138, 110.0792,  96.3231,  90.2322,\n",
      "          94.9772,  88.0948,  95.5470,  88.3441,  92.7931,   1.3467,   1.8852,\n",
      "         184.6552,  16.5562, 184.0349,  35.2706]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 87.1823,  97.1128, 101.5982, 117.5979, 115.5824,  93.8905, 106.9655,\n",
      "          99.9854,  91.7273, 109.8873,  99.9236,  90.8806,   5.0248,   3.0187,\n",
      "         161.2382,  10.5669, 119.9536,  23.1577]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[185.3415, 183.3734, 194.7859, 173.4577, 170.0484, 199.4192, 174.9852,\n",
      "         184.2457, 192.3940, 170.8158, 195.1151, 200.2727,  12.9895,   8.0903,\n",
      "         159.5181,   2.2665, 166.5328,   7.1571]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[170.5139, 179.8052, 185.2793, 170.5000, 168.7476, 182.2724, 169.1534,\n",
      "         175.8705, 183.8721, 174.3668, 181.0385, 181.5280,  16.1655,   7.5988,\n",
      "         127.1855,   2.2231, 165.3930,   7.6034]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[144.6516, 147.8305, 145.1608, 145.3560, 154.1574, 131.7404, 156.6112,\n",
      "         144.6839, 140.7207, 156.2130, 141.1035, 129.4347,   8.8361,   6.6321,\n",
      "          99.3793,   4.0983,  89.2193,  11.5478]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[186.9887, 196.7114, 171.8672, 171.1920, 173.6098, 181.8882, 174.4379,\n",
      "         172.2551, 192.5135, 174.3240, 181.8516, 186.6855,  18.4148,  11.7692,\n",
      "          72.3481,   2.3762,  70.0053,   7.0877]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[148.7166, 168.5066, 145.2065, 150.6226, 160.9164, 136.4503, 163.3026,\n",
      "         145.5698, 159.0286, 165.4832, 143.5026, 137.2422,  12.4657,   8.2598,\n",
      "          64.0958,   3.0196,  76.8055,   9.6376]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[193.1502, 189.0345, 169.7618, 169.6732, 176.2460, 176.7584, 178.4139,\n",
      "         181.6789, 187.6559, 174.6407, 175.8907, 179.0203,  18.5674,  10.6637,\n",
      "          41.0500,   2.9023,  68.9649,   8.6107]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[218.3911, 209.9024, 193.1727, 184.4071, 194.1525, 205.1568, 189.0661,\n",
      "         200.0496, 214.3433, 187.4520, 206.2108, 209.7356,  19.5474,  12.6958,\n",
      "          46.2200,   1.8878,  64.8666,   6.2852]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[199.0510, 177.0699, 178.5939, 177.1283, 168.0883, 191.2343, 176.1425,\n",
      "         185.0553, 189.6200, 173.5373, 185.1940, 194.9854,  19.0212,  10.7315,\n",
      "         113.2348,   3.5440, 108.2235,   9.9574]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 87.4366,  93.2120,  95.7852, 113.5667, 105.1071,  94.3404,  95.8380,\n",
      "          93.9522,  89.2421,  99.8143,  87.8130,  94.6264,   3.3396,   2.7070,\n",
      "         205.8871,  12.1040, 194.5348,  26.6553]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 90.3363,  88.1522,  98.6180, 121.3602, 132.0102,  84.2243, 121.5467,\n",
      "          97.2362,  85.1398, 115.6695,  99.5214,  79.7654,   2.9757,   2.6117,\n",
      "         162.8592,  12.5133, 125.2607,  26.9068]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[119.5281, 112.7718, 138.7221, 133.7684, 134.6087, 123.2281, 137.0180,\n",
      "         129.4485, 118.0857, 135.5165, 122.7636, 116.6091,   6.6817,   3.4509,\n",
      "         174.9872,   6.1986, 138.8093,  14.7068]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[109.5213, 104.5999, 126.1309, 119.3128, 109.2695, 125.0667, 116.9407,\n",
      "         126.2750, 110.7513, 111.4432, 114.4767, 120.4912,   1.7671,   1.9201,\n",
      "         208.6101,   9.3484, 230.1896,  22.6205]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[136.0644, 130.2340, 145.1451, 141.8312, 148.3033, 125.5836, 166.7201,\n",
      "         145.7545, 124.9490, 161.5542, 136.1903, 118.4920,   6.9031,   5.7136,\n",
      "         131.2041,   3.6600,  99.5057,  10.4313]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[113.0756, 104.3715, 128.3481, 122.1380, 111.1741, 125.9933, 118.4776,\n",
      "         121.4610, 113.0003, 117.8814, 110.2180, 124.3502,   3.0312,   2.7909,\n",
      "         224.7243,   9.2903, 224.1358,  20.7252]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(dset)):\n",
    "    inputs = torch.Tensor(dset[i]).reshape(1,896,1536)\n",
    "    predictions = mlp_model(inputs.transpose(1,2))\n",
    "    print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 1d CNN + Max Pooling + Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPModelPooling(nn.Module):\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super(MLPModelPooling, self).__init__(*args, **kwargs)\n",
    "\n",
    "        self.layers = nn.Sequential(OrderedDict([\n",
    "            ('conv1x1', nn.Conv1d(1536, 500, 1)),\n",
    "            ('gelu1', nn.GELU()),\n",
    "            ('maxpool1', nn.MaxPool1d(896)),\n",
    "            ('flatten', nn.Flatten()),\n",
    "            ('fc1', nn.Linear(500, 18))\n",
    "        ]))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPModelPooling(\n",
       "  (layers): Sequential(\n",
       "    (conv1x1): Conv1d(1536, 500, kernel_size=(1,), stride=(1,))\n",
       "    (gelu1): GELU(approximate='none')\n",
       "    (maxpool1): MaxPool1d(kernel_size=896, stride=896, padding=0, dilation=1, ceil_mode=False)\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "    (fc1): Linear(in_features=500, out_features=18, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool_model = MLPModelPooling()\n",
    "pool_model.load_state_dict(torch.load('../cs282a_perceptron-maxpool/model_20231128_072156_3'))\n",
    "pool_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[187.6417, 182.9631, 172.3025, 171.0777, 177.0425, 178.5789, 174.7300,\n",
      "         177.8415, 186.8805, 172.4592, 180.7357, 180.9333,  16.4236,  11.8089,\n",
      "          96.5649,   4.4737,  74.6781,  10.4619]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[167.4127, 174.4773, 159.5704, 157.5585, 155.1230, 167.7140, 155.4463,\n",
      "         158.8519, 173.2176, 157.5184, 164.5945, 172.0140,  14.6018,  10.5576,\n",
      "         115.2723,   3.8817,  91.8531,   8.5334]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[128.7370, 131.3702, 131.6922, 136.2482, 134.5864, 123.2989, 139.5895,\n",
      "         132.2911, 127.2798, 139.2241, 126.8751, 122.9097,   7.8661,   5.7660,\n",
      "         150.1024,   6.3933, 123.9708,  14.5684]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[101.0909,  97.1315, 115.4432, 117.8345, 105.8991, 114.1162, 112.4615,\n",
      "         119.6248, 104.3572, 105.1508, 104.3456, 112.9376,   2.9304,   3.1745,\n",
      "         226.8024,  11.2567, 221.3595,  22.9852]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 87.5435,  77.3884,  94.4952, 111.0738, 109.6526,  89.0814, 100.5512,\n",
      "         101.3215,  81.1793,  97.9590,  88.1699,  85.8451,   3.2101,   1.8188,\n",
      "         202.9724,  15.6792, 179.1847,  30.0323]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 98.2159,  96.3023, 103.3186, 118.6384, 112.0555, 103.2584, 104.4891,\n",
      "         106.2457,  99.6968, 104.4200,  98.5800, 102.6744,   4.7478,   3.5781,\n",
      "         187.9804,  12.6404, 146.6187,  24.4528]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[180.1973, 180.6148, 178.5123, 170.8288, 168.6101, 182.9467, 171.1076,\n",
      "         177.3102, 184.1223, 168.4846, 180.4881, 185.7298,  13.7542,   9.8964,\n",
      "         152.9317,   4.1541, 145.3132,   9.9809]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[174.7357, 179.4171, 174.6931, 170.5544, 167.7291, 178.4687, 166.7888,\n",
      "         172.4681, 182.0278, 168.2093, 174.9695, 181.9120,  13.6494,   9.0527,\n",
      "         165.3162,   4.9473, 153.1187,  11.4145]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[133.6029, 137.8104, 131.3112, 141.7047, 147.0689, 124.9251, 144.5155,\n",
      "         131.9893, 133.5288, 145.1384, 132.3727, 123.8779,   7.9582,   6.4394,\n",
      "         121.6602,   6.0671,  89.7058,  14.1977]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[200.3499, 196.8369, 180.9803, 179.2801, 184.9886, 192.7051, 182.1259,\n",
      "         187.6960, 200.6216, 180.4433, 192.2855, 196.3240,  17.8431,  13.0455,\n",
      "          94.3734,   4.3416,  76.5220,   9.7266]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[151.3914, 160.2703, 145.8406, 155.1182, 157.8336, 147.0563, 149.8684,\n",
      "         145.1445, 159.1357, 155.9415, 150.7827, 150.0501,  13.1040,  10.1316,\n",
      "         111.7724,   5.7187,  68.1377,  13.0097]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[184.2954, 183.2325, 170.2293, 164.9913, 170.8724, 176.4577, 169.5755,\n",
      "         173.3117, 184.5413, 170.0425, 177.8307, 180.4055,  15.4346,  11.3812,\n",
      "          86.7632,   3.9196,  69.7097,   9.2232]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[204.3212, 203.7320, 185.0232, 181.9815, 189.2925, 194.4804, 184.3990,\n",
      "         190.1322, 205.2128, 185.7348, 196.1974, 198.3800,  18.5307,  13.6204,\n",
      "          74.0625,   4.0450,  61.2065,   8.8217]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[197.4836, 197.2118, 183.6775, 178.4234, 179.5610, 194.7724, 178.4184,\n",
      "         186.4863, 199.8148, 178.6778, 191.3294, 198.8082,  16.6400,  11.7001,\n",
      "         115.5351,   4.0738, 107.1987,   8.2682]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 84.6855,  81.8025,  94.3365, 112.1173, 106.1643,  88.7659,  98.6661,\n",
      "          97.6492,  83.8112,  98.5730,  85.0885,  85.9792,   2.5912,   1.8572,\n",
      "         205.9133,  12.9844, 191.1396,  26.5592]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[110.4886,  98.3671, 109.0127, 123.8610, 127.5357,  94.6987, 131.0074,\n",
      "         118.2859,  95.4926, 126.7998, 105.1682,  87.8522,   4.4143,   3.9404,\n",
      "         151.6283,   9.9484, 119.9106,  21.0930]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[123.4877, 122.3575, 128.8844, 138.7863, 137.7039, 121.2939, 133.5522,\n",
      "         128.3707, 122.7035, 134.5501, 122.2847, 119.6901,   7.3667,   4.9992,\n",
      "         168.8611,   8.8371, 134.6183,  19.9914]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[107.6625, 106.6761, 125.9963, 124.7503, 113.0159, 123.6706, 114.9618,\n",
      "         124.4157, 113.5421, 111.5231, 114.0670, 122.8183,   3.1894,   1.7713,\n",
      "         235.0997,  10.5224, 242.6677,  21.7031]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[146.1293, 138.5386, 143.0321, 145.0443, 147.8304, 132.1585, 157.6723,\n",
      "         148.9701, 134.1007, 152.1175, 141.0420, 127.4002,   9.2165,   6.9733,\n",
      "         124.9544,   5.8221, 103.7994,  12.1260]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[102.4301, 102.4761, 123.4420, 125.4943, 111.8942, 113.9818, 118.2751,\n",
      "         117.8369, 102.9706, 116.5529, 110.3985, 112.5181,   2.9891,   1.6784,\n",
      "         248.1535,   9.3988, 223.8605,  19.9221]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(dset)):\n",
    "    inputs = torch.Tensor(dset[i]).reshape(1,896,1536)\n",
    "    predictions = pool_model(inputs.transpose(1,2))\n",
    "    print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, d_model, heads, forward_expansion, dropout, max_length):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=d_model, num_heads=heads, dropout=dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, forward_expansion * d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(forward_expansion * d_model, d_model)\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Additional linear layer for output transformation\n",
    "        self.output_transform = nn.Linear(d_model, 18)\n",
    "\n",
    "        # Adaptive pooling layer to handle sequence length\n",
    "        self.sequence_pooling = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "    def forward(self, x, enc_out=None, src_mask=None, trg_mask=None):\n",
    "        attention_output, _ = self.attention(x, x, x, attn_mask=trg_mask)\n",
    "        query = self.dropout(self.norm1(attention_output + x))\n",
    "\n",
    "        out = self.feed_forward(query)\n",
    "        out = self.dropout(self.norm2(out + query))\n",
    "\n",
    "        out_transformed = self.output_transform(out)\n",
    "\n",
    "        out_pooled = self.sequence_pooling(out_transformed.transpose(1, 2)).transpose(1, 2)\n",
    "\n",
    "        return out_pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerDecoder(\n",
       "  (attention): MultiheadAttention(\n",
       "    (out_proj): NonDynamicallyQuantizableLinear(in_features=1536, out_features=1536, bias=True)\n",
       "  )\n",
       "  (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "  (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "  (feed_forward): Sequential(\n",
       "    (0): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "    (3): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "  )\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (output_transform): Linear(in_features=1536, out_features=18, bias=True)\n",
       "  (sequence_pooling): AdaptiveAvgPool1d(output_size=1)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_basenji_transformer = TransformerDecoder(d_model=1536, heads=6, forward_expansion=2, dropout=0.2, max_length=896)\n",
    "trained_filepath  = '../cs282a_self-attention/model_20231128_080512_7'\n",
    "trained_basenji_transformer.load_state_dict(torch.load(trained_filepath))\n",
    "trained_basenji_transformer.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[192.1179, 150.0023, 172.0852, 163.9631, 183.5785, 166.4874, 183.9130,\n",
      "          188.9735, 162.8639, 170.7770, 177.2941, 156.4945,  17.7902,  10.1166,\n",
      "           76.4997,   5.4054,  74.5288,  11.7602]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "tensor([[[151.0708, 179.3533, 151.1176, 154.5428, 150.8366, 157.7057, 142.9166,\n",
      "          144.1196, 171.6321, 152.7803, 152.4716, 163.7518,  16.7840,  10.1042,\n",
      "          117.5346,   3.4626, 103.1409,   9.8003]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "tensor([[[130.2856, 147.0574, 146.2804, 143.7901, 145.7762, 128.9431, 154.5514,\n",
      "          136.5906, 134.7770, 159.8839, 138.5390, 124.8242,  10.4996,   5.6922,\n",
      "          157.1832,   4.0811, 139.3578,  10.1028]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "tensor([[[ 98.6346, 100.6480, 113.1690, 117.4675, 112.4247, 112.6396, 109.6569,\n",
      "          111.0721, 103.6091, 102.4488, 107.0185, 110.5289,   4.1152,   2.7382,\n",
      "          218.2716,  11.6817, 227.7730,  24.4503]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "tensor([[[ 76.1707,  71.6956,  80.8957, 108.5665, 108.0301,  84.9272,  90.0249,\n",
      "           85.6574,  77.6185,  80.2861,  81.0685,  82.5986,   2.7845,   2.5072,\n",
      "          208.4245,  18.3689, 188.2905,  34.4084]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "tensor([[[ 89.8874,  89.7588,  96.0257, 116.9055, 114.5035,  90.8725, 106.6342,\n",
      "           97.2983,  88.4080, 102.5183,  92.2102,  88.7232,   4.9521,   3.6815,\n",
      "          180.8283,  13.1216, 137.0209,  24.7346]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "tensor([[[193.6989, 172.6130, 196.4724, 169.6981, 173.9314, 203.2726, 169.7765,\n",
      "          191.2867, 193.4335, 161.6913, 194.0898, 202.4600,  18.3401,   8.8359,\n",
      "          158.0431,   3.5199, 174.8571,  10.9095]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "tensor([[[188.8190, 178.9431, 185.3893, 171.8509, 177.7944, 187.4969, 172.4972,\n",
      "          184.4634, 186.6426, 170.2650, 187.9866, 185.8093,  18.6246,   8.9295,\n",
      "          133.6667,   2.7488, 156.5637,   8.8619]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "tensor([[[146.5343, 142.1894, 142.1726, 142.3781, 154.5397, 128.8860, 163.3170,\n",
      "          150.4011, 135.3694, 157.1847, 144.1035, 124.8177,  11.0009,   8.1728,\n",
      "          106.2687,   5.4818,  87.4276,  11.7302]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "tensor([[[189.8468, 188.8370, 171.4068, 166.4960, 173.0548, 179.9799, 170.3404,\n",
      "          177.8133, 188.8929, 170.2686, 180.6801, 183.1401,  20.6783,  12.7656,\n",
      "           77.4466,   2.5329,  70.9689,   7.8485]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "tensor([[[150.7613, 169.8611, 143.0742, 144.3237, 150.6234, 134.5233, 164.4312,\n",
      "          150.1356, 150.3138, 165.7671, 145.9371, 133.7478,  13.7958,  10.5246,\n",
      "           60.8929,   2.4750,  69.1970,   8.1036]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "tensor([[[196.8324, 178.2233, 162.2458, 160.3179, 171.5342, 170.0042, 172.1790,\n",
      "          187.2568, 176.8430, 164.7650, 175.0260, 168.6044,  18.9934,  12.0615,\n",
      "           26.1296,   4.4618,  69.4776,  11.1283]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "tensor([[[225.9146, 195.6206, 187.8718, 176.7342, 186.6360, 205.1748, 185.7681,\n",
      "          207.5763, 204.4830, 178.0914, 202.4979, 205.2175,  24.6935,  13.8720,\n",
      "           44.5605,   3.1434,  68.1145,   9.1406]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "tensor([[[199.5742, 163.2379, 182.0650, 163.3365, 161.7494, 192.9762, 165.3759,\n",
      "          190.0871, 179.4297, 156.8117, 182.9788, 191.0386,  20.0299,  10.6123,\n",
      "          123.0201,   3.7630, 120.4666,  10.6488]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "tensor([[[ 77.9007,  83.7686,  83.3643, 109.7495, 104.9143,  86.8789,  93.1360,\n",
      "           86.7538,  81.7057,  87.8707,  83.8700,  86.0511,   2.8121,   2.4233,\n",
      "          220.2180,  15.3069, 201.9286,  29.4460]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "tensor([[[ 80.4133,  73.8677,  88.6626, 117.2687, 129.8382,  77.1895, 113.7204,\n",
      "           89.2269,  76.8142, 103.2488,  91.5655,  72.9473,   3.6753,   3.0139,\n",
      "          184.8166,  15.0387, 119.7824,  26.6827]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "tensor([[[117.4281, 105.4406, 138.4756, 129.2857, 131.8386, 122.7125, 139.4209,\n",
      "          130.6634, 113.7950, 132.9493, 121.6702, 114.6462,   6.7951,   4.1862,\n",
      "          185.9472,   8.6703, 138.6519,  17.9567]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "tensor([[[107.4956,  95.2038, 124.1847, 116.1672, 110.6242, 124.1727, 113.7351,\n",
      "          122.6266, 106.7316, 100.7134, 113.0545, 119.6194,   3.9773,   2.4906,\n",
      "          222.5149,  11.3051, 240.4522,  24.5701]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "tensor([[[140.0662, 123.6677, 147.3643, 137.7935, 145.2414, 123.6395, 166.3697,\n",
      "          148.1782, 119.6409, 160.4519, 137.5892, 114.3492,   9.5548,   6.4161,\n",
      "          140.4273,   4.9589,  89.9793,  10.6555]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "tensor([[[105.5697, 100.0836, 123.3179, 117.6654, 109.9597, 123.5745, 114.1763,\n",
      "          119.6878, 108.3474, 104.5026, 111.8016, 120.3860,   4.1260,   2.8118,\n",
      "          233.9371,  10.6367, 232.6159,  23.2062]]],\n",
      "       grad_fn=<TransposeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(dset)):\n",
    "    inputs = torch.Tensor(dset[i]).reshape(1,896,1536)\n",
    "    predictions = trained_basenji_transformer(inputs)\n",
    "    print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs282a_test",
   "language": "python",
   "name": "cs282a_test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
