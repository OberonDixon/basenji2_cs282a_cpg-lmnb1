{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4608c74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ds_util import get_dataset\n",
    "from model import MLPModel\n",
    "import numpy as np\n",
    "import torch\n",
    "import h5py\n",
    "from datetime import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b75eb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File('/clusterfs/nilah/oberon/datasets/basenji/embeddings/embeddings.h5','r')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2be31a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = f['embeddings']\n",
    "\n",
    "train_inds = np.zeros(len(dset), dtype=bool)\n",
    "val_inds = np.zeros(len(dset), dtype=bool)\n",
    "test_inds = np.zeros(len(dset), dtype=bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a90be92",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inds[:34021]=True\n",
    "val_inds[34021:36234]=True\n",
    "test_inds[36234:]=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9df2dc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_full = h5py.File('/clusterfs/nilah/oberon/datasets/cs282a/dataset_14-lmnb1_4-cpg.h5')['single_bin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8ae0325",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pearson_correlation(prediction, target):\n",
    "    # Flatten the tensors to 1D\n",
    "    prediction_flat = prediction.view(-1).cpu().detach().numpy()\n",
    "    target_flat = target.view(-1).cpu().detach().numpy()\n",
    "\n",
    "    # Calculate Pearson's correlation\n",
    "    corr, _ = pearsonr(prediction_flat, target_flat)\n",
    "    return corr\n",
    "\n",
    "def spearman_correlation(prediction,target):\n",
    "    # Flatten the tensors to 1D\n",
    "    prediction_flat = prediction.view(-1).cpu().detach().numpy()\n",
    "    target_flat = target.view(-1).cpu().detach().numpy()\n",
    "\n",
    "    # Calculate Spearman's correlation\n",
    "    corr, _ = spearmanr(prediction_flat, target_flat)\n",
    "    return corr    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04d3d017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting at  2023-11-28 06:54:34.427071\n",
      "0:01:34.659593\n",
      "0:02:54.480318\n",
      "0:27:20.080791\n",
      "0:27:20.097129\n",
      "0:27:20.102772\n",
      "0:27:20.102925\n"
     ]
    }
   ],
   "source": [
    "start_load = datetime.now()\n",
    "print('starting at ',start_load)\n",
    "val_tensor_dset = torch.utils.data.TensorDataset(torch.Tensor(dset[val_inds]), torch.Tensor(labels_full[val_inds]))\n",
    "print(datetime.now()-start_load)\n",
    "test_tensor_dset = torch.utils.data.TensorDataset(torch.Tensor(dset[test_inds]), torch.Tensor(labels_full[test_inds]))\n",
    "print(datetime.now()-start_load)\n",
    "train_tensor_dset = torch.utils.data.TensorDataset(torch.Tensor(dset[train_inds]), torch.Tensor(labels_full[train_inds]))\n",
    "print(datetime.now()-start_load)\n",
    "training_loader = torch.utils.data.DataLoader(train_tensor_dset, batch_size=4, shuffle=True)\n",
    "print(datetime.now()-start_load)\n",
    "validation_loader = torch.utils.data.DataLoader(val_tensor_dset, batch_size=4, shuffle=False)\n",
    "print(datetime.now()-start_load)\n",
    "test_loader = torch.utils.data.DataLoader(test_tensor_dset, batch_size=4, shuffle=False)\n",
    "print(datetime.now()-start_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "464a3cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_model = MLPModel()\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(curr_model.parameters(), lr=0.000005, momentum=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64387239",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index, tb_writer):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for i, data in enumerate(training_loader):\n",
    "        inputs, labels = data\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = curr_model(inputs.transpose(1,2))\n",
    "\n",
    "        loss = loss_fn(outputs, labels.squeeze(1))\n",
    "        loss.backward()\n",
    "        # Apply torch.nan_to_num to gradients\n",
    "        for param in curr_model.parameters():\n",
    "            if param.grad is not None:\n",
    "                param.grad = torch.nan_to_num(param.grad)\n",
    "                \n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        all_predictions.append(torch.nan_to_num(outputs).detach())\n",
    "        all_labels.append(torch.nan_to_num(labels).detach())\n",
    "\n",
    "\n",
    "        \n",
    "        if i % 250 == 249:\n",
    "            last_loss = running_loss / 250\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            tb_x = epoch_index * len(training_loader) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.\n",
    "            predictions_flat = torch.cat(all_predictions).view(-1)\n",
    "            labels_flat = torch.cat(all_labels).view(-1)\n",
    "            pcorr = pearson_correlation(predictions_flat, labels_flat)\n",
    "            scorr = spearman_correlation(predictions_flat,labels_flat)\n",
    "            print(\"  batch {} Pearson correlation: {}, Spearman correlation: {}\".format(i + 1, pcorr.item(),scorr.item()))\n",
    "            \n",
    "            all_predictions = []\n",
    "            all_labels = []\n",
    "            \n",
    "            \n",
    "\n",
    "    return last_loss, pcorr, scorr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fafcdcbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1\n",
      "-------------------------------\n",
      "  batch 250 loss: 6258.500887939453\n",
      "  batch 250 Pearson correlation: 0.4885842766722295, Spearman correlation: 0.41591395644651225\n",
      "  batch 500 loss: 1528.721437133789\n",
      "  batch 500 Pearson correlation: 0.8308102605076149, Spearman correlation: 0.7209943193161956\n",
      "  batch 750 loss: 1267.768804321289\n",
      "  batch 750 Pearson correlation: 0.8596376582232973, Spearman correlation: 0.7696116168773964\n",
      "  batch 1000 loss: 1102.4890606689453\n",
      "  batch 1000 Pearson correlation: 0.8790601997473338, Spearman correlation: 0.808449254492517\n",
      "  batch 1250 loss: 971.8797003173828\n",
      "  batch 1250 Pearson correlation: 0.892697735618405, Spearman correlation: 0.8254352372660414\n",
      "  batch 1500 loss: 927.9501573486328\n",
      "  batch 1500 Pearson correlation: 0.8977073741979487, Spearman correlation: 0.8257128052160804\n",
      "  batch 1750 loss: 920.7919936523438\n",
      "  batch 1750 Pearson correlation: 0.8972298209505021, Spearman correlation: 0.8289878942558431\n",
      "  batch 2000 loss: 851.7142744140625\n",
      "  batch 2000 Pearson correlation: 0.9084349851098742, Spearman correlation: 0.8516024474244843\n",
      "  batch 2250 loss: 799.0030560302735\n",
      "  batch 2250 Pearson correlation: 0.9102486877115468, Spearman correlation: 0.849948143513526\n",
      "  batch 2500 loss: 805.6500443115234\n",
      "  batch 2500 Pearson correlation: 0.912882725098296, Spearman correlation: 0.8632164960636625\n",
      "  batch 2750 loss: 738.8565737915039\n",
      "  batch 2750 Pearson correlation: 0.920184063406291, Spearman correlation: 0.86977800767368\n",
      "  batch 3000 loss: 735.7181619873047\n",
      "  batch 3000 Pearson correlation: 0.9187339295906307, Spearman correlation: 0.8634187142880194\n",
      "  batch 3250 loss: 749.9879487915039\n",
      "  batch 3250 Pearson correlation: 0.9180630329371433, Spearman correlation: 0.8646661846218358\n",
      "  batch 3500 loss: 709.2999016723633\n",
      "  batch 3500 Pearson correlation: 0.921863427343532, Spearman correlation: 0.873822766778813\n",
      "  batch 3750 loss: 715.8369193115234\n",
      "  batch 3750 Pearson correlation: 0.9203644332205347, Spearman correlation: 0.8717002793611944\n",
      "  batch 4000 loss: 718.8203801879882\n",
      "  batch 4000 Pearson correlation: 0.9218149411186156, Spearman correlation: 0.8757774210553212\n",
      "  batch 4250 loss: 687.8575289306641\n",
      "  batch 4250 Pearson correlation: 0.925398925775005, Spearman correlation: 0.8814112964761096\n",
      "  batch 4500 loss: nan\n",
      "  batch 4500 Pearson correlation: 0.9293424219810454, Spearman correlation: 0.8864075684964488\n",
      "  batch 4750 loss: 673.2397523803711\n",
      "  batch 4750 Pearson correlation: 0.9279197428507653, Spearman correlation: 0.8889492654598427\n",
      "  batch 5000 loss: 659.7375655517578\n",
      "  batch 5000 Pearson correlation: 0.9276485084582216, Spearman correlation: 0.8905497563209388\n",
      "  batch 5250 loss: 623.4307564086914\n",
      "  batch 5250 Pearson correlation: 0.9316405949972243, Spearman correlation: 0.8900167089928593\n",
      "  batch 5500 loss: 643.9994169921875\n",
      "  batch 5500 Pearson correlation: 0.931507646392352, Spearman correlation: 0.8906976860228331\n",
      "  batch 5750 loss: 623.9879785766601\n",
      "  batch 5750 Pearson correlation: 0.9321078841034693, Spearman correlation: 0.8972179789394891\n",
      "  batch 6000 loss: 623.4196326293945\n",
      "  batch 6000 Pearson correlation: 0.9322238831183409, Spearman correlation: 0.8962489496017089\n",
      "  batch 6250 loss: 639.8211479492187\n",
      "  batch 6250 Pearson correlation: 0.9303804194104459, Spearman correlation: 0.8870804308283814\n",
      "  batch 6500 loss: 619.9400795898438\n",
      "  batch 6500 Pearson correlation: 0.9327236516338213, Spearman correlation: 0.890980873725538\n",
      "  batch 6750 loss: 611.0821274414062\n",
      "  batch 6750 Pearson correlation: 0.9339250841628949, Spearman correlation: 0.8960462946728754\n",
      "  batch 7000 loss: 601.6490946655274\n",
      "  batch 7000 Pearson correlation: 0.9356401414143244, Spearman correlation: 0.8982486273600261\n",
      "  batch 7250 loss: 585.0211240234376\n",
      "  batch 7250 Pearson correlation: 0.936931406816656, Spearman correlation: 0.9012089111452934\n",
      "  batch 7500 loss: 571.02121875\n",
      "  batch 7500 Pearson correlation: 0.9379272558829743, Spearman correlation: 0.9032163689617566\n",
      "  batch 7750 loss: 600.3128135375977\n",
      "  batch 7750 Pearson correlation: 0.9340084798127952, Spearman correlation: 0.899464646976556\n",
      "  batch 8000 loss: 623.4783330078125\n",
      "  batch 8000 Pearson correlation: 0.9337149766827744, Spearman correlation: 0.8947161574720179\n",
      "  batch 8250 loss: 572.2073913574219\n",
      "  batch 8250 Pearson correlation: 0.9374243064628882, Spearman correlation: 0.903432718871246\n",
      "  batch 8500 loss: 572.5514799804688\n",
      "  batch 8500 Pearson correlation: 0.9372973816603205, Spearman correlation: 0.9008030417153318\n",
      "LOSS train 572.5514799804688 valid 680.6358642578125\n",
      "Validation Pearson Correlation: 0.932736811381833, Spearman Correlation: 0.8945205164632815\n",
      "EPOCH 2\n",
      "-------------------------------\n",
      "  batch 250 loss: 563.0373145751953\n",
      "  batch 250 Pearson correlation: 0.938503294010977, Spearman correlation: 0.9014084399754411\n",
      "  batch 500 loss: 547.7548115234375\n",
      "  batch 500 Pearson correlation: 0.9398851230609082, Spearman correlation: 0.9061629857074911\n",
      "  batch 750 loss: 561.018189453125\n",
      "  batch 750 Pearson correlation: 0.93994499367496, Spearman correlation: 0.9076113657534824\n",
      "  batch 1000 loss: 575.7937387695313\n",
      "  batch 1000 Pearson correlation: 0.9377387749343656, Spearman correlation: 0.9024192551556153\n",
      "  batch 1250 loss: 553.1283372192382\n",
      "  batch 1250 Pearson correlation: 0.9404410820629621, Spearman correlation: 0.9121965865788162\n",
      "  batch 1500 loss: 558.6956490478516\n",
      "  batch 1500 Pearson correlation: 0.9381867975183145, Spearman correlation: 0.904568289775168\n",
      "  batch 1750 loss: 566.1048110961914\n",
      "  batch 1750 Pearson correlation: 0.9393328447852154, Spearman correlation: 0.9027036121191129\n",
      "  batch 2000 loss: 564.9260289916992\n",
      "  batch 2000 Pearson correlation: 0.9379479573852652, Spearman correlation: 0.9029358346370691\n",
      "  batch 2250 loss: 557.5132565307617\n",
      "  batch 2250 Pearson correlation: 0.9394238575614386, Spearman correlation: 0.904797122630915\n",
      "  batch 2500 loss: 557.9770780029297\n",
      "  batch 2500 Pearson correlation: 0.9405473571894026, Spearman correlation: 0.906399132303063\n",
      "  batch 2750 loss: 558.5468229370117\n",
      "  batch 2750 Pearson correlation: 0.9398254192160539, Spearman correlation: 0.9052711301801839\n",
      "  batch 3000 loss: 549.3331011352539\n",
      "  batch 3000 Pearson correlation: 0.9411245245414159, Spearman correlation: 0.9095855201452689\n",
      "  batch 3250 loss: 553.7509362182617\n",
      "  batch 3250 Pearson correlation: 0.9410854462726385, Spearman correlation: 0.9097658001786442\n",
      "  batch 3500 loss: 527.5512111816406\n",
      "  batch 3500 Pearson correlation: 0.9435224049738662, Spearman correlation: 0.9129346582045821\n",
      "  batch 3750 loss: 536.9024185180664\n",
      "  batch 3750 Pearson correlation: 0.9417897061594561, Spearman correlation: 0.9106001489662279\n",
      "  batch 4000 loss: 533.1530172729492\n",
      "  batch 4000 Pearson correlation: 0.9426309810069233, Spearman correlation: 0.9102479182546991\n",
      "  batch 4250 loss: 551.1853836669922\n",
      "  batch 4250 Pearson correlation: 0.9407849588110451, Spearman correlation: 0.903557574568657\n",
      "  batch 4500 loss: 552.2117623901368\n",
      "  batch 4500 Pearson correlation: 0.9410409754756458, Spearman correlation: 0.9105804310056266\n",
      "  batch 4750 loss: 538.5775858764648\n",
      "  batch 4750 Pearson correlation: 0.941785474745444, Spearman correlation: 0.909250945014639\n",
      "  batch 5000 loss: 543.6301867675782\n",
      "  batch 5000 Pearson correlation: 0.9412130907441991, Spearman correlation: 0.9095452642357102\n",
      "  batch 5250 loss: 523.5650137634277\n",
      "  batch 5250 Pearson correlation: 0.9430039801790292, Spearman correlation: 0.910876627443715\n",
      "  batch 5500 loss: 527.9002666015625\n",
      "  batch 5500 Pearson correlation: 0.9431315635692212, Spearman correlation: 0.9125687959199899\n",
      "  batch 5750 loss: 536.9613645629883\n",
      "  batch 5750 Pearson correlation: 0.9416445312522451, Spearman correlation: 0.9078626300969704\n",
      "  batch 6000 loss: 505.5723359375\n",
      "  batch 6000 Pearson correlation: 0.944644202702228, Spearman correlation: 0.9147188810542916\n",
      "  batch 6250 loss: 521.3331526489258\n",
      "  batch 6250 Pearson correlation: 0.9430050612242795, Spearman correlation: 0.9122068535609342\n",
      "  batch 6500 loss: nan\n",
      "  batch 6500 Pearson correlation: 0.9437379561776739, Spearman correlation: 0.9106555560284827\n",
      "  batch 6750 loss: 494.44557067871096\n",
      "  batch 6750 Pearson correlation: 0.9456756765903159, Spearman correlation: 0.9155710088732459\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 7000 loss: 535.1036639404297\n",
      "  batch 7000 Pearson correlation: 0.9425185073643791, Spearman correlation: 0.9122064434266535\n",
      "  batch 7250 loss: 504.6178358154297\n",
      "  batch 7250 Pearson correlation: 0.9455133195706107, Spearman correlation: 0.9166542642065639\n",
      "  batch 7500 loss: 506.56117993164065\n",
      "  batch 7500 Pearson correlation: 0.9457576360238681, Spearman correlation: 0.913790573669175\n",
      "  batch 7750 loss: 559.8061134643555\n",
      "  batch 7750 Pearson correlation: 0.9408207443896559, Spearman correlation: 0.9116657101295798\n",
      "  batch 8000 loss: 503.13255047607424\n",
      "  batch 8000 Pearson correlation: 0.9463194630345492, Spearman correlation: 0.9179569730048922\n",
      "  batch 8250 loss: 519.1504570007324\n",
      "  batch 8250 Pearson correlation: 0.9436020770489084, Spearman correlation: 0.9127364729756144\n",
      "  batch 8500 loss: 519.7018712768555\n",
      "  batch 8500 Pearson correlation: 0.9436869552669198, Spearman correlation: 0.9147040410992655\n",
      "LOSS train 519.7018712768555 valid 636.3772583007812\n",
      "Validation Pearson Correlation: 0.9376565620432809, Spearman Correlation: 0.9039814078263012\n",
      "EPOCH 3\n",
      "-------------------------------\n",
      "  batch 250 loss: 515.6271361083984\n",
      "  batch 250 Pearson correlation: 0.9447163750913136, Spearman correlation: 0.9146465519144746\n",
      "  batch 500 loss: 513.3359405517579\n",
      "  batch 500 Pearson correlation: 0.945962501470794, Spearman correlation: 0.9155210125409824\n",
      "  batch 750 loss: 498.6715953979492\n",
      "  batch 750 Pearson correlation: 0.9453232650069707, Spearman correlation: 0.917242654280016\n",
      "  batch 1000 loss: 514.1317500610352\n",
      "  batch 1000 Pearson correlation: 0.9448954246241118, Spearman correlation: 0.9160615221889773\n",
      "  batch 1250 loss: 500.44887878417967\n",
      "  batch 1250 Pearson correlation: 0.9458607408157786, Spearman correlation: 0.9136153029588509\n",
      "  batch 1500 loss: 508.6492863769531\n",
      "  batch 1500 Pearson correlation: 0.9458191019658093, Spearman correlation: 0.9140675408383795\n",
      "  batch 1750 loss: 491.529604309082\n",
      "  batch 1750 Pearson correlation: 0.9463712587365549, Spearman correlation: 0.91719474792558\n",
      "  batch 2000 loss: 498.7200413818359\n",
      "  batch 2000 Pearson correlation: 0.9450414565439279, Spearman correlation: 0.9130103999484817\n",
      "  batch 2250 loss: 487.6806614990234\n",
      "  batch 2250 Pearson correlation: 0.9471243779152693, Spearman correlation: 0.9192202832204175\n",
      "  batch 2500 loss: 481.8523570556641\n",
      "  batch 2500 Pearson correlation: 0.9478593278609514, Spearman correlation: 0.9168342724653263\n",
      "  batch 2750 loss: 516.6867875976562\n",
      "  batch 2750 Pearson correlation: 0.9443336482912529, Spearman correlation: 0.9152377489506022\n",
      "  batch 3000 loss: 486.26147314453124\n",
      "  batch 3000 Pearson correlation: 0.9467907084701476, Spearman correlation: 0.9182666267004298\n",
      "  batch 3250 loss: 497.27500494384765\n",
      "  batch 3250 Pearson correlation: 0.9465122470395222, Spearman correlation: 0.9163746076352915\n",
      "  batch 3500 loss: 473.26387084960936\n",
      "  batch 3500 Pearson correlation: 0.9489123045947891, Spearman correlation: 0.9207972167764659\n",
      "  batch 3750 loss: 510.95797662353516\n",
      "  batch 3750 Pearson correlation: 0.945633939781331, Spearman correlation: 0.9166470378443641\n",
      "  batch 4000 loss: 500.6522344970703\n",
      "  batch 4000 Pearson correlation: 0.9458782499962758, Spearman correlation: 0.9194822877024486\n",
      "  batch 4250 loss: 516.3633011474609\n",
      "  batch 4250 Pearson correlation: 0.9444610532424276, Spearman correlation: 0.9133261334765874\n",
      "  batch 4500 loss: nan\n",
      "  batch 4500 Pearson correlation: 0.9461158869127092, Spearman correlation: 0.9149516175019935\n",
      "  batch 4750 loss: 498.13551000976565\n",
      "  batch 4750 Pearson correlation: 0.9471252078045453, Spearman correlation: 0.9201434394259301\n",
      "  batch 5000 loss: 514.397631652832\n",
      "  batch 5000 Pearson correlation: 0.9445650355734524, Spearman correlation: 0.9145435361265549\n",
      "  batch 5250 loss: 504.32279486083985\n",
      "  batch 5250 Pearson correlation: 0.9458661346696914, Spearman correlation: 0.9148347393093706\n",
      "  batch 5500 loss: 504.4954760131836\n",
      "  batch 5500 Pearson correlation: 0.9457647254736271, Spearman correlation: 0.9196621509633832\n",
      "  batch 5750 loss: 492.9672268066406\n",
      "  batch 5750 Pearson correlation: 0.9470827022009513, Spearman correlation: 0.9173852650486932\n",
      "  batch 6000 loss: 481.0554930419922\n",
      "  batch 6000 Pearson correlation: 0.9482916627749662, Spearman correlation: 0.9182046468262751\n",
      "  batch 6250 loss: 478.2907908935547\n",
      "  batch 6250 Pearson correlation: 0.9484306931623135, Spearman correlation: 0.9221718662447106\n",
      "  batch 6500 loss: 475.3434157714844\n",
      "  batch 6500 Pearson correlation: 0.9487744873294118, Spearman correlation: 0.9226345462485702\n",
      "  batch 6750 loss: 508.485741394043\n",
      "  batch 6750 Pearson correlation: 0.9441278993664108, Spearman correlation: 0.9111535073236937\n",
      "  batch 7000 loss: 497.4878884277344\n",
      "  batch 7000 Pearson correlation: 0.9463674349541988, Spearman correlation: 0.9184535450476522\n",
      "  batch 7250 loss: 491.80676953125\n",
      "  batch 7250 Pearson correlation: 0.9478146192264307, Spearman correlation: 0.9181243842264427\n",
      "  batch 7500 loss: 479.9589356689453\n",
      "  batch 7500 Pearson correlation: 0.9478642258552382, Spearman correlation: 0.9206564524179035\n",
      "  batch 7750 loss: 478.78482653808595\n",
      "  batch 7750 Pearson correlation: 0.9474907699407814, Spearman correlation: 0.9211860756644219\n",
      "  batch 8000 loss: 496.05806195068357\n",
      "  batch 8000 Pearson correlation: 0.9460090669917434, Spearman correlation: 0.9184604635975259\n",
      "  batch 8250 loss: 486.73286029052736\n",
      "  batch 8250 Pearson correlation: 0.9479606453500569, Spearman correlation: 0.9178618803032397\n",
      "  batch 8500 loss: 481.900543762207\n",
      "  batch 8500 Pearson correlation: 0.947620109119762, Spearman correlation: 0.9177554508763093\n",
      "LOSS train 481.900543762207 valid 597.1056518554688\n",
      "Validation Pearson Correlation: 0.9413385045250151, Spearman Correlation: 0.9091419272684197\n",
      "EPOCH 4\n",
      "-------------------------------\n",
      "  batch 250 loss: 498.0352932128906\n",
      "  batch 250 Pearson correlation: 0.9477099894097742, Spearman correlation: 0.9225959647267198\n",
      "  batch 500 loss: 487.21264935302736\n",
      "  batch 500 Pearson correlation: 0.9483602376482249, Spearman correlation: 0.9222130856292008\n",
      "  batch 750 loss: 477.12279040527346\n",
      "  batch 750 Pearson correlation: 0.9479899310039318, Spearman correlation: 0.9212345621279586\n",
      "  batch 1000 loss: 475.25045739746093\n",
      "  batch 1000 Pearson correlation: 0.9474899400570165, Spearman correlation: 0.9185703986676231\n",
      "  batch 1250 loss: 468.88623693847654\n",
      "  batch 1250 Pearson correlation: 0.9492938598028955, Spearman correlation: 0.9233962501015699\n",
      "  batch 1500 loss: nan\n",
      "  batch 1500 Pearson correlation: 0.9490997474716233, Spearman correlation: 0.9215359210213704\n",
      "  batch 1750 loss: 495.73705596923827\n",
      "  batch 1750 Pearson correlation: 0.9473739733543945, Spearman correlation: 0.9180792347867747\n",
      "  batch 2000 loss: 476.9312004699707\n",
      "  batch 2000 Pearson correlation: 0.9492536215677619, Spearman correlation: 0.9215813288591842\n",
      "  batch 2250 loss: 471.8527556152344\n",
      "  batch 2250 Pearson correlation: 0.9489174300047891, Spearman correlation: 0.924753645650946\n",
      "  batch 2500 loss: 471.2061798095703\n",
      "  batch 2500 Pearson correlation: 0.9484983585815167, Spearman correlation: 0.9218392302700653\n",
      "  batch 2750 loss: 473.5277479248047\n",
      "  batch 2750 Pearson correlation: 0.9481149198191459, Spearman correlation: 0.9202015177430777\n",
      "  batch 3000 loss: 478.0493900756836\n",
      "  batch 3000 Pearson correlation: 0.9482845824622703, Spearman correlation: 0.9207211690204433\n",
      "  batch 3250 loss: 465.7099817504883\n",
      "  batch 3250 Pearson correlation: 0.9492384108263365, Spearman correlation: 0.9194711996392136\n",
      "  batch 3500 loss: 467.33198165893555\n",
      "  batch 3500 Pearson correlation: 0.9496035240120545, Spearman correlation: 0.9228490099073506\n",
      "  batch 3750 loss: 468.8251950683594\n",
      "  batch 3750 Pearson correlation: 0.9495705577773554, Spearman correlation: 0.9183353149243777\n",
      "  batch 4000 loss: 491.9131541137695\n",
      "  batch 4000 Pearson correlation: 0.9465765729742712, Spearman correlation: 0.9164724042491564\n",
      "  batch 4250 loss: 486.71940435791015\n",
      "  batch 4250 Pearson correlation: 0.9468536157759765, Spearman correlation: 0.9178247065203927\n",
      "  batch 4500 loss: 472.2415255126953\n",
      "  batch 4500 Pearson correlation: 0.9485396541463306, Spearman correlation: 0.9232076428600133\n",
      "  batch 4750 loss: 473.70475537109377\n",
      "  batch 4750 Pearson correlation: 0.9488017852146512, Spearman correlation: 0.9180762389356434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 5000 loss: 486.3881654052734\n",
      "  batch 5000 Pearson correlation: 0.9476562173682305, Spearman correlation: 0.9183542274551276\n",
      "  batch 5250 loss: 495.434389251709\n",
      "  batch 5250 Pearson correlation: 0.9469473385461082, Spearman correlation: 0.915395268735309\n",
      "  batch 5500 loss: 469.77723889160154\n",
      "  batch 5500 Pearson correlation: 0.9495135318380995, Spearman correlation: 0.9247631637810705\n",
      "  batch 5750 loss: 486.8671820678711\n",
      "  batch 5750 Pearson correlation: 0.9479753012068003, Spearman correlation: 0.9182951485855863\n",
      "  batch 6000 loss: 479.1629215698242\n",
      "  batch 6000 Pearson correlation: 0.9485871576052153, Spearman correlation: 0.9192041927362243\n",
      "  batch 6250 loss: 468.47275384521487\n",
      "  batch 6250 Pearson correlation: 0.9496198345955711, Spearman correlation: 0.9214520818407796\n",
      "  batch 6500 loss: 471.9086069335938\n",
      "  batch 6500 Pearson correlation: 0.9486781681167544, Spearman correlation: 0.9210683678867916\n",
      "  batch 6750 loss: 475.11847619628907\n",
      "  batch 6750 Pearson correlation: 0.9482348869166712, Spearman correlation: 0.9176018801935921\n",
      "  batch 7000 loss: 467.9209140625\n",
      "  batch 7000 Pearson correlation: 0.9505416424862237, Spearman correlation: 0.9227853336653438\n",
      "  batch 7250 loss: 468.9887590332031\n",
      "  batch 7250 Pearson correlation: 0.9498388626806501, Spearman correlation: 0.92436677317716\n",
      "  batch 7500 loss: 455.7596123046875\n",
      "  batch 7500 Pearson correlation: 0.9520496772768648, Spearman correlation: 0.9263452776827215\n",
      "  batch 7750 loss: 444.43145074462893\n",
      "  batch 7750 Pearson correlation: 0.9511374492884322, Spearman correlation: 0.9260730216611943\n",
      "  batch 8000 loss: 473.44862109375\n",
      "  batch 8000 Pearson correlation: 0.9494703472653658, Spearman correlation: 0.9233011452672776\n",
      "  batch 8250 loss: 460.8389758911133\n",
      "  batch 8250 Pearson correlation: 0.950404159354132, Spearman correlation: 0.9270596734345475\n",
      "  batch 8500 loss: 492.79570764160155\n",
      "  batch 8500 Pearson correlation: 0.9470584580304022, Spearman correlation: 0.9192885341816951\n",
      "LOSS train 492.79570764160155 valid 567.9296264648438\n",
      "Validation Pearson Correlation: 0.9436369145938305, Spearman Correlation: 0.912128601300309\n",
      "EPOCH 5\n",
      "-------------------------------\n",
      "  batch 250 loss: 473.1179422607422\n",
      "  batch 250 Pearson correlation: 0.9485636351919009, Spearman correlation: 0.9223041741371022\n",
      "  batch 500 loss: 480.00553018188475\n",
      "  batch 500 Pearson correlation: 0.9475522132713211, Spearman correlation: 0.9222829210366864\n",
      "  batch 750 loss: 469.864419128418\n",
      "  batch 750 Pearson correlation: 0.9492817739126437, Spearman correlation: 0.920701662016833\n",
      "  batch 1000 loss: 464.3774812927246\n",
      "  batch 1000 Pearson correlation: 0.9501793909927634, Spearman correlation: 0.9269746221994887\n",
      "  batch 1250 loss: 457.87276251220703\n",
      "  batch 1250 Pearson correlation: 0.950784854894164, Spearman correlation: 0.9254239779918821\n",
      "  batch 1500 loss: 472.3844556274414\n",
      "  batch 1500 Pearson correlation: 0.9480263241700996, Spearman correlation: 0.9181974491380325\n",
      "  batch 1750 loss: 445.85618267822264\n",
      "  batch 1750 Pearson correlation: 0.9512797523622505, Spearman correlation: 0.9235157056427257\n",
      "  batch 2000 loss: 457.42120806884765\n",
      "  batch 2000 Pearson correlation: 0.9503779122805008, Spearman correlation: 0.9241858290279433\n",
      "  batch 2250 loss: 456.15199896240233\n",
      "  batch 2250 Pearson correlation: 0.9515105492186184, Spearman correlation: 0.9271197756654105\n",
      "  batch 2500 loss: 466.9909782104492\n",
      "  batch 2500 Pearson correlation: 0.9489226427311571, Spearman correlation: 0.9201545768592398\n",
      "  batch 2750 loss: 468.23531149291995\n",
      "  batch 2750 Pearson correlation: 0.9500465558557166, Spearman correlation: 0.9245790927291863\n",
      "  batch 3000 loss: 470.2020103149414\n",
      "  batch 3000 Pearson correlation: 0.9491496263988014, Spearman correlation: 0.9210113014112451\n",
      "  batch 3250 loss: 467.40403466796874\n",
      "  batch 3250 Pearson correlation: 0.9500514747479627, Spearman correlation: 0.9229107078006645\n",
      "  batch 3500 loss: 468.37841174316407\n",
      "  batch 3500 Pearson correlation: 0.9500722103316286, Spearman correlation: 0.9236611679020754\n",
      "  batch 3750 loss: 463.0275435791016\n",
      "  batch 3750 Pearson correlation: 0.9496699132309228, Spearman correlation: 0.9182105967989905\n",
      "  batch 4000 loss: 463.71647271728517\n",
      "  batch 4000 Pearson correlation: 0.9497508139118158, Spearman correlation: 0.9200555738454341\n",
      "  batch 4250 loss: 449.9459497070313\n",
      "  batch 4250 Pearson correlation: 0.9515413016836036, Spearman correlation: 0.9252267329453168\n",
      "  batch 4500 loss: 443.13020428466797\n",
      "  batch 4500 Pearson correlation: 0.9517205454921246, Spearman correlation: 0.9264116740046854\n",
      "  batch 4750 loss: nan\n",
      "  batch 4750 Pearson correlation: 0.9514234966156092, Spearman correlation: 0.9245863407428102\n",
      "  batch 5000 loss: 455.45227487182615\n",
      "  batch 5000 Pearson correlation: 0.9511127472047511, Spearman correlation: 0.9302246940193861\n",
      "  batch 5250 loss: 423.0112606201172\n",
      "  batch 5250 Pearson correlation: 0.9535644761978987, Spearman correlation: 0.928049601711113\n",
      "  batch 5500 loss: 484.8926577148437\n",
      "  batch 5500 Pearson correlation: 0.9473543363084255, Spearman correlation: 0.9191613389682735\n",
      "  batch 5750 loss: 480.5310331420898\n",
      "  batch 5750 Pearson correlation: 0.9497476374121736, Spearman correlation: 0.9238100630754366\n",
      "  batch 6000 loss: 470.05767779541014\n",
      "  batch 6000 Pearson correlation: 0.9504602609914735, Spearman correlation: 0.9223092637183765\n",
      "  batch 6250 loss: 461.5997321166992\n",
      "  batch 6250 Pearson correlation: 0.9505796818600665, Spearman correlation: 0.9254042734645365\n",
      "  batch 6500 loss: 462.33978228759764\n",
      "  batch 6500 Pearson correlation: 0.9508061396563806, Spearman correlation: 0.9230852095448737\n",
      "  batch 6750 loss: 436.3011948852539\n",
      "  batch 6750 Pearson correlation: 0.9524028626965702, Spearman correlation: 0.9288350878084078\n",
      "  batch 7000 loss: 452.7014342346191\n",
      "  batch 7000 Pearson correlation: 0.9515450145313753, Spearman correlation: 0.9253616938194364\n",
      "  batch 7250 loss: 448.9047370605469\n",
      "  batch 7250 Pearson correlation: 0.9524303541001226, Spearman correlation: 0.9253306756641675\n",
      "  batch 7500 loss: 442.84497149658205\n",
      "  batch 7500 Pearson correlation: 0.9522448640370731, Spearman correlation: 0.9266389965242326\n",
      "  batch 7750 loss: 471.80393536376954\n",
      "  batch 7750 Pearson correlation: 0.9491225281507333, Spearman correlation: 0.921946982963808\n",
      "  batch 8000 loss: 466.2013848876953\n",
      "  batch 8000 Pearson correlation: 0.9494513695261907, Spearman correlation: 0.920306904765723\n",
      "  batch 8250 loss: 475.26961990356443\n",
      "  batch 8250 Pearson correlation: 0.9497180015862388, Spearman correlation: 0.9218894907492216\n",
      "  batch 8500 loss: 446.8102593383789\n",
      "  batch 8500 Pearson correlation: 0.9523795608791822, Spearman correlation: 0.9289113618717284\n",
      "LOSS train 446.8102593383789 valid 569.0692749023438\n",
      "Validation Pearson Correlation: 0.943461525933796, Spearman Correlation: 0.9127458289255737\n"
     ]
    }
   ],
   "source": [
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/mlp_model_{}'.format(timestamp))\n",
    "\n",
    "epoch_number = 0\n",
    "EPOCHS = 5\n",
    "best_vloss = 1_000_000.\n",
    "\n",
    "epoch_loss, epoch_coeff = [], []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}\\n-------------------------------'.format(epoch_number + 1))\n",
    "\n",
    "    curr_model.train(True)\n",
    "    avg_loss,pcorr,scorr = train_one_epoch(epoch_number, writer)\n",
    "    running_vloss = 0.0\n",
    "    curr_model.eval()\n",
    "    \n",
    "    all_vpredictions = []\n",
    "    all_vlabels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, vdata in enumerate(validation_loader):\n",
    "            vinputs, vlabels = vdata\n",
    "            voutputs = curr_model(vinputs.transpose(1,2))\n",
    "            vloss = loss_fn(voutputs, vlabels.squeeze(1))\n",
    "            running_vloss += vloss\n",
    "            \n",
    "            all_vpredictions.append(voutputs)\n",
    "            all_vlabels.append(vlabels)\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    \n",
    "    predictions_flat = torch.cat(all_vpredictions).view(-1)\n",
    "    labels_flat = torch.cat(all_vlabels).view(-1)\n",
    "    pval_corr = pearson_correlation(predictions_flat, labels_flat)\n",
    "    sval_corr = spearman_correlation(predictions_flat,labels_flat)\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "    print('Validation Pearson Correlation: {}, Spearman Correlation: {}'.format(pval_corr.item(),sval_corr.item()))\n",
    "    \n",
    "    writer.add_scalars('Training vs. Validation Loss',\n",
    "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                    epoch_number + 1)\n",
    "    writer.flush()\n",
    "\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n",
    "        torch.save(curr_model.state_dict(), model_path)\n",
    "\n",
    "    epoch_loss.append((avg_loss, avg_vloss))\n",
    "    epoch_coeff.append((scorr, sval_corr))\n",
    "    \n",
    "    epoch_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "461f8960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.9008030417153318, 0.8945205164632815), (0.9147040410992655, 0.9039814078263012), (0.9177554508763093, 0.9091419272684197), (0.9192885341816951, 0.912128601300309), (0.9289113618717284, 0.9127458289255737)]\n"
     ]
    }
   ],
   "source": [
    "print(epoch_coeff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95a7da2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_cs282a_mlp",
   "language": "python",
   "name": "pytorch_cs282a_mlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
