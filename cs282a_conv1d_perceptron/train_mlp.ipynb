{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea011cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ds_util import get_dataset\n",
    "from perceptron_model import MLPModel\n",
    "import numpy as np\n",
    "import torch\n",
    "import h5py\n",
    "from datetime import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a1b6b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File('/clusterfs/nilah/oberon/datasets/basenji/embeddings/embeddings.h5','r')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6623fa87",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = f['embeddings']\n",
    "\n",
    "train_inds = np.zeros(len(dset), dtype=bool)\n",
    "val_inds = np.zeros(len(dset), dtype=bool)\n",
    "test_inds = np.zeros(len(dset), dtype=bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "529e430d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inds[:34021]=True\n",
    "val_inds[34021:36234]=True\n",
    "test_inds[36234:]=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eac821c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_full = h5py.File('/clusterfs/nilah/oberon/datasets/cs282a/dataset_14-lmnb1_4-cpg.h5')['single_bin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "836cc537",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pearson_correlation(prediction, target):\n",
    "    # Flatten the tensors to 1D\n",
    "    prediction_flat = prediction.view(-1).cpu().detach().numpy()\n",
    "    target_flat = target.view(-1).cpu().detach().numpy()\n",
    "\n",
    "    # Calculate Pearson's correlation\n",
    "    corr, _ = pearsonr(prediction_flat, target_flat)\n",
    "    return corr\n",
    "\n",
    "def spearman_correlation(prediction,target):\n",
    "    # Flatten the tensors to 1D\n",
    "    prediction_flat = prediction.view(-1).cpu().detach().numpy()\n",
    "    target_flat = target.view(-1).cpu().detach().numpy()\n",
    "\n",
    "    # Calculate Spearman's correlation\n",
    "    corr, _ = spearmanr(prediction_flat, target_flat)\n",
    "    return corr    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed0f945e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting at  2023-11-27 19:37:59.952691\n",
      "0:01:35.625081\n",
      "0:02:52.873978\n",
      "0:27:54.498324\n",
      "0:27:54.503382\n",
      "0:27:54.503570\n",
      "0:27:54.504362\n"
     ]
    }
   ],
   "source": [
    "start_load = datetime.now()\n",
    "print('starting at ',start_load)\n",
    "val_tensor_dset = torch.utils.data.TensorDataset(torch.Tensor(dset[val_inds]), torch.Tensor(labels_full[val_inds]))\n",
    "print(datetime.now()-start_load)\n",
    "test_tensor_dset = torch.utils.data.TensorDataset(torch.Tensor(dset[test_inds]), torch.Tensor(labels_full[test_inds]))\n",
    "print(datetime.now()-start_load)\n",
    "train_tensor_dset = torch.utils.data.TensorDataset(torch.Tensor(dset[train_inds]), torch.Tensor(labels_full[train_inds]))\n",
    "print(datetime.now()-start_load)\n",
    "training_loader = torch.utils.data.DataLoader(train_tensor_dset, batch_size=4, shuffle=True)\n",
    "print(datetime.now()-start_load)\n",
    "validation_loader = torch.utils.data.DataLoader(val_tensor_dset, batch_size=4, shuffle=False)\n",
    "print(datetime.now()-start_load)\n",
    "test_loader = torch.utils.data.DataLoader(test_tensor_dset, batch_size=4, shuffle=False)\n",
    "print(datetime.now()-start_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e751211",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_model = MLPModel()\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(curr_model.parameters(), lr=0.000005, momentum=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae15f977",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index, tb_writer):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for i, data in enumerate(training_loader):\n",
    "        inputs, labels = data\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = curr_model(inputs.transpose(1,2))\n",
    "\n",
    "        loss = loss_fn(outputs, labels.squeeze(1))\n",
    "        loss.backward()\n",
    "        # Apply torch.nan_to_num to gradients\n",
    "        for param in curr_model.parameters():\n",
    "            if param.grad is not None:\n",
    "                param.grad = torch.nan_to_num(param.grad)\n",
    "                \n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        all_predictions.append(torch.nan_to_num(outputs).detach())\n",
    "        all_labels.append(torch.nan_to_num(labels).detach())\n",
    "\n",
    "\n",
    "        \n",
    "        if i % 250 == 249:\n",
    "            last_loss = running_loss / 250\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            tb_x = epoch_index * len(training_loader) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.\n",
    "            predictions_flat = torch.cat(all_predictions).view(-1)\n",
    "            labels_flat = torch.cat(all_labels).view(-1)\n",
    "            pcorr = pearson_correlation(predictions_flat, labels_flat)\n",
    "            scorr = spearman_correlation(predictions_flat,labels_flat)\n",
    "            print(\"  batch {} Pearson correlation: {}, Spearman correlation: {}\".format(i + 1, pcorr.item(),scorr.item()))\n",
    "\n",
    "            all_predictions = []\n",
    "            all_labels = []\n",
    "            \n",
    "            \n",
    "\n",
    "    return last_loss, pcorr, scorr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "94bf5665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1\n",
      "-------------------------------\n",
      "  batch 250 loss: 3279.786724243164\n",
      "  batch 250 Pearson correlation: 0.6894281268379249, Spearman correlation: 0.6523257050599491\n",
      "  batch 500 loss: 538.5323609008789\n",
      "  batch 500 Pearson correlation: 0.9412819787657906, Spearman correlation: 0.9079783060343478\n",
      "  batch 750 loss: 514.6707052001954\n",
      "  batch 750 Pearson correlation: 0.9443298605854294, Spearman correlation: 0.9147097326673045\n",
      "  batch 1000 loss: nan\n",
      "  batch 1000 Pearson correlation: 0.948649805484788, Spearman correlation: 0.9212098150777837\n",
      "  batch 1250 loss: 488.90921905517575\n",
      "  batch 1250 Pearson correlation: 0.9476468753485154, Spearman correlation: 0.9185681405327344\n",
      "  batch 1500 loss: 465.7970531005859\n",
      "  batch 1500 Pearson correlation: 0.949775832025745, Spearman correlation: 0.9211608049084798\n",
      "  batch 1750 loss: 464.2482590332031\n",
      "  batch 1750 Pearson correlation: 0.9505877611041958, Spearman correlation: 0.9236150204008549\n",
      "  batch 2000 loss: 473.7239056396484\n",
      "  batch 2000 Pearson correlation: 0.949333283312009, Spearman correlation: 0.922609506666874\n",
      "  batch 2250 loss: 441.7418223876953\n",
      "  batch 2250 Pearson correlation: 0.953665339827862, Spearman correlation: 0.9302251686902061\n",
      "  batch 2500 loss: 447.59638580322263\n",
      "  batch 2500 Pearson correlation: 0.9513194037806781, Spearman correlation: 0.9244358735959936\n",
      "  batch 2750 loss: 437.1873749084473\n",
      "  batch 2750 Pearson correlation: 0.9526512132799234, Spearman correlation: 0.9271279630793138\n",
      "  batch 3000 loss: 455.00687017822264\n",
      "  batch 3000 Pearson correlation: 0.9511074860126791, Spearman correlation: 0.9264689150717825\n",
      "  batch 3250 loss: 436.5080973815918\n",
      "  batch 3250 Pearson correlation: 0.9529982893975004, Spearman correlation: 0.9283748283779462\n",
      "  batch 3500 loss: 438.8046239929199\n",
      "  batch 3500 Pearson correlation: 0.9520725262137266, Spearman correlation: 0.9255392342425435\n",
      "  batch 3750 loss: 422.49309088134765\n",
      "  batch 3750 Pearson correlation: 0.9546331555140772, Spearman correlation: 0.9304271790791642\n",
      "  batch 4000 loss: 416.30263436889646\n",
      "  batch 4000 Pearson correlation: 0.954419523275708, Spearman correlation: 0.9295382468692063\n",
      "  batch 4250 loss: 433.8122162475586\n",
      "  batch 4250 Pearson correlation: 0.9537980976175551, Spearman correlation: 0.9308090731194775\n",
      "  batch 4500 loss: 420.73005297851563\n",
      "  batch 4500 Pearson correlation: 0.9546112948451533, Spearman correlation: 0.9302543396473099\n",
      "  batch 4750 loss: 426.4306092224121\n",
      "  batch 4750 Pearson correlation: 0.9539477397268907, Spearman correlation: 0.9282952687295138\n",
      "  batch 5000 loss: 443.64219470214846\n",
      "  batch 5000 Pearson correlation: 0.9517343378450227, Spearman correlation: 0.9249454232002207\n",
      "  batch 5250 loss: 433.08736740112306\n",
      "  batch 5250 Pearson correlation: 0.953869119302446, Spearman correlation: 0.9294640713993325\n",
      "  batch 5500 loss: 417.514368560791\n",
      "  batch 5500 Pearson correlation: 0.9551731633122147, Spearman correlation: 0.9346480253485783\n",
      "  batch 5750 loss: 400.8215995178223\n",
      "  batch 5750 Pearson correlation: 0.9563840583746264, Spearman correlation: 0.9349270647431508\n",
      "  batch 6000 loss: 446.8190474243164\n",
      "  batch 6000 Pearson correlation: 0.9519432861646295, Spearman correlation: 0.9254485218338441\n",
      "  batch 6250 loss: 421.4713482666016\n",
      "  batch 6250 Pearson correlation: 0.95485242220607, Spearman correlation: 0.9308396161819519\n",
      "  batch 6500 loss: 416.7139734802246\n",
      "  batch 6500 Pearson correlation: 0.9551162190569947, Spearman correlation: 0.9296983123266319\n",
      "  batch 6750 loss: 415.807447265625\n",
      "  batch 6750 Pearson correlation: 0.9549796223562051, Spearman correlation: 0.9313829160377063\n",
      "  batch 7000 loss: 432.8467569580078\n",
      "  batch 7000 Pearson correlation: 0.9540246114245657, Spearman correlation: 0.9305757792674391\n",
      "  batch 7250 loss: 422.3247504272461\n",
      "  batch 7250 Pearson correlation: 0.9544537502004338, Spearman correlation: 0.9298597040498823\n",
      "  batch 7500 loss: 419.6293354797363\n",
      "  batch 7500 Pearson correlation: 0.9551774311095603, Spearman correlation: 0.9324567790252664\n",
      "  batch 7750 loss: 423.8372945556641\n",
      "  batch 7750 Pearson correlation: 0.9552816719383519, Spearman correlation: 0.9346821441573342\n",
      "  batch 8000 loss: 415.7694310913086\n",
      "  batch 8000 Pearson correlation: 0.9547859625658999, Spearman correlation: 0.9287248256978243\n",
      "  batch 8250 loss: 452.019215637207\n",
      "  batch 8250 Pearson correlation: 0.95171350726999, Spearman correlation: 0.9286137989241472\n",
      "  batch 8500 loss: 434.65453527832034\n",
      "  batch 8500 Pearson correlation: 0.9539672006206705, Spearman correlation: 0.930538185999887\n",
      "LOSS train 434.65453527832034 valid 504.4618225097656\n",
      "Validation Pearson Correlation: 0.9499259940515379, Spearman Correlation: 0.9225518642702144\n",
      "EPOCH 2\n",
      "-------------------------------\n",
      "  batch 250 loss: 407.1689884643555\n",
      "  batch 250 Pearson correlation: 0.9558466422733913, Spearman correlation: 0.9333646409065085\n",
      "  batch 500 loss: 400.39156060791015\n",
      "  batch 500 Pearson correlation: 0.9575904873819391, Spearman correlation: 0.9348657632454169\n",
      "  batch 750 loss: 399.7843568115234\n",
      "  batch 750 Pearson correlation: 0.9571452445213887, Spearman correlation: 0.9347255915003506\n",
      "  batch 1000 loss: 409.47788601684573\n",
      "  batch 1000 Pearson correlation: 0.9555071624470169, Spearman correlation: 0.9310121931364671\n",
      "  batch 1250 loss: 413.40317498779297\n",
      "  batch 1250 Pearson correlation: 0.9552430406584285, Spearman correlation: 0.9319849462710244\n",
      "  batch 1500 loss: 416.6594758911133\n",
      "  batch 1500 Pearson correlation: 0.9553906184043516, Spearman correlation: 0.9325684177687661\n",
      "  batch 1750 loss: 415.13031100463866\n",
      "  batch 1750 Pearson correlation: 0.9543930367443444, Spearman correlation: 0.9322734077420082\n",
      "  batch 2000 loss: 414.3837209167481\n",
      "  batch 2000 Pearson correlation: 0.9549965130693503, Spearman correlation: 0.9310082835273193\n",
      "  batch 2250 loss: 400.362790435791\n",
      "  batch 2250 Pearson correlation: 0.9571002459072833, Spearman correlation: 0.9366968967574718\n",
      "  batch 2500 loss: 401.86451333618163\n",
      "  batch 2500 Pearson correlation: 0.9566328850984638, Spearman correlation: 0.9313959870809965\n",
      "  batch 2750 loss: 417.8803771057129\n",
      "  batch 2750 Pearson correlation: 0.9553746904632737, Spearman correlation: 0.9301161315984313\n",
      "  batch 3000 loss: 402.66879400634764\n",
      "  batch 3000 Pearson correlation: 0.9575225496370756, Spearman correlation: 0.9365688019311168\n",
      "  batch 3250 loss: 401.19897637939454\n",
      "  batch 3250 Pearson correlation: 0.9566579283871206, Spearman correlation: 0.9346254176587766\n",
      "  batch 3500 loss: 396.5688547668457\n",
      "  batch 3500 Pearson correlation: 0.9565127704919707, Spearman correlation: 0.9312948746351103\n",
      "  batch 3750 loss: 393.27689666748046\n",
      "  batch 3750 Pearson correlation: 0.9578599086515037, Spearman correlation: 0.9345426397169013\n",
      "  batch 4000 loss: 418.4832389526367\n",
      "  batch 4000 Pearson correlation: 0.9550967841411774, Spearman correlation: 0.9305688849077548\n",
      "  batch 4250 loss: 405.6047493286133\n",
      "  batch 4250 Pearson correlation: 0.9560328387323963, Spearman correlation: 0.9328848870134365\n",
      "  batch 4500 loss: 402.4515947570801\n",
      "  batch 4500 Pearson correlation: 0.9573404076805874, Spearman correlation: 0.9330877818247764\n",
      "  batch 4750 loss: 399.3175230407715\n",
      "  batch 4750 Pearson correlation: 0.9578016568845735, Spearman correlation: 0.9376390012860483\n",
      "  batch 5000 loss: 406.5423047485352\n",
      "  batch 5000 Pearson correlation: 0.956152507142577, Spearman correlation: 0.930059079920921\n",
      "  batch 5250 loss: 388.23740560913086\n",
      "  batch 5250 Pearson correlation: 0.9581367588026506, Spearman correlation: 0.9366248454360738\n",
      "  batch 5500 loss: 420.1309312744141\n",
      "  batch 5500 Pearson correlation: 0.954986591557845, Spearman correlation: 0.9346829137655691\n",
      "  batch 5750 loss: 395.2455462036133\n",
      "  batch 5750 Pearson correlation: 0.9575393676835874, Spearman correlation: 0.9346632130308622\n",
      "  batch 6000 loss: 414.03939733886716\n",
      "  batch 6000 Pearson correlation: 0.9559747313736999, Spearman correlation: 0.9332950952829352\n",
      "  batch 6250 loss: 411.10808142089843\n",
      "  batch 6250 Pearson correlation: 0.9568570170222027, Spearman correlation: 0.9341531188197003\n",
      "  batch 6500 loss: 423.35248760986326\n",
      "  batch 6500 Pearson correlation: 0.9557664237763384, Spearman correlation: 0.9337939492970162\n",
      "  batch 6750 loss: 405.1914874267578\n",
      "  batch 6750 Pearson correlation: 0.9573736469676181, Spearman correlation: 0.9372722443078076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 7000 loss: 392.3660195007324\n",
      "  batch 7000 Pearson correlation: 0.9573841366447077, Spearman correlation: 0.9333758817968569\n",
      "  batch 7250 loss: 407.49762774658205\n",
      "  batch 7250 Pearson correlation: 0.9555485332182743, Spearman correlation: 0.9313954613067474\n",
      "  batch 7500 loss: 414.966072265625\n",
      "  batch 7500 Pearson correlation: 0.9554544053193875, Spearman correlation: 0.9311305866807961\n",
      "  batch 7750 loss: 429.2652268066406\n",
      "  batch 7750 Pearson correlation: 0.9540675841169267, Spearman correlation: 0.9314227650907891\n",
      "  batch 8000 loss: nan\n",
      "  batch 8000 Pearson correlation: 0.9571080840176451, Spearman correlation: 0.9319939117522714\n",
      "  batch 8250 loss: 389.50419952392576\n",
      "  batch 8250 Pearson correlation: 0.9583958631622511, Spearman correlation: 0.9386249382587175\n",
      "  batch 8500 loss: 406.2132159423828\n",
      "  batch 8500 Pearson correlation: 0.9559802432578623, Spearman correlation: 0.9327588409721504\n",
      "LOSS train 406.2132159423828 valid 506.2987365722656\n",
      "Validation Pearson Correlation: 0.9495169830653356, Spearman Correlation: 0.9228575848560653\n",
      "EPOCH 3\n",
      "-------------------------------\n",
      "  batch 250 loss: 408.6897549133301\n",
      "  batch 250 Pearson correlation: 0.9559179579124991, Spearman correlation: 0.931998816548837\n",
      "  batch 500 loss: 412.5465267944336\n",
      "  batch 500 Pearson correlation: 0.9562204442043969, Spearman correlation: 0.9349154066348848\n",
      "  batch 750 loss: 401.4995824890137\n",
      "  batch 750 Pearson correlation: 0.957265203266159, Spearman correlation: 0.9345834392744556\n",
      "  batch 1000 loss: 400.2860625305176\n",
      "  batch 1000 Pearson correlation: 0.9577301907004553, Spearman correlation: 0.9342281392753571\n",
      "  batch 1250 loss: 394.8284475708008\n",
      "  batch 1250 Pearson correlation: 0.9574673668831843, Spearman correlation: 0.9362271823672063\n",
      "  batch 1500 loss: 396.72366400146484\n",
      "  batch 1500 Pearson correlation: 0.9574675469148574, Spearman correlation: 0.9356304689680094\n",
      "  batch 1750 loss: 389.7248695678711\n",
      "  batch 1750 Pearson correlation: 0.9576831985153991, Spearman correlation: 0.9368332423342189\n",
      "  batch 2000 loss: 398.75188259887693\n",
      "  batch 2000 Pearson correlation: 0.9572726076743041, Spearman correlation: 0.9350709722515621\n",
      "  batch 2250 loss: 379.85234753417967\n",
      "  batch 2250 Pearson correlation: 0.9596660247044375, Spearman correlation: 0.936357554456201\n",
      "  batch 2500 loss: 397.5562919006348\n",
      "  batch 2500 Pearson correlation: 0.956825216445062, Spearman correlation: 0.9358902442217635\n",
      "  batch 2750 loss: 388.9507859802246\n",
      "  batch 2750 Pearson correlation: 0.9578215181162976, Spearman correlation: 0.9330223137778112\n",
      "  batch 3000 loss: 389.448807434082\n",
      "  batch 3000 Pearson correlation: 0.9575742515945149, Spearman correlation: 0.9358460681252994\n",
      "  batch 3250 loss: 409.96704483032227\n",
      "  batch 3250 Pearson correlation: 0.9568065259730402, Spearman correlation: 0.9374583617327091\n",
      "  batch 3500 loss: 394.11061029052735\n",
      "  batch 3500 Pearson correlation: 0.9583662465309818, Spearman correlation: 0.935719214856996\n",
      "  batch 3750 loss: 387.2953387451172\n",
      "  batch 3750 Pearson correlation: 0.9580331902839812, Spearman correlation: 0.9330843605110704\n",
      "  batch 4000 loss: 403.7533134155273\n",
      "  batch 4000 Pearson correlation: 0.9572255798308424, Spearman correlation: 0.9349755182893487\n",
      "  batch 4250 loss: 398.47632104492186\n",
      "  batch 4250 Pearson correlation: 0.9573866957384406, Spearman correlation: 0.9345832215356391\n",
      "  batch 4500 loss: 387.59362246704103\n",
      "  batch 4500 Pearson correlation: 0.9579952443979249, Spearman correlation: 0.939488562842014\n",
      "  batch 4750 loss: 393.5470731201172\n",
      "  batch 4750 Pearson correlation: 0.9583266183939202, Spearman correlation: 0.9389224815976168\n",
      "  batch 5000 loss: 392.70146881103517\n",
      "  batch 5000 Pearson correlation: 0.9580169559092985, Spearman correlation: 0.9356207067155234\n",
      "  batch 5250 loss: 380.402776763916\n",
      "  batch 5250 Pearson correlation: 0.9586330301196122, Spearman correlation: 0.9369573325509692\n",
      "  batch 5500 loss: 392.3408427124023\n",
      "  batch 5500 Pearson correlation: 0.9575253935134888, Spearman correlation: 0.9327180920386379\n",
      "  batch 5750 loss: 387.0036679077148\n",
      "  batch 5750 Pearson correlation: 0.9578962520917389, Spearman correlation: 0.9376410967722761\n",
      "  batch 6000 loss: 415.10855310058594\n",
      "  batch 6000 Pearson correlation: 0.9555644937154553, Spearman correlation: 0.9286900828076906\n",
      "  batch 6250 loss: 393.2609524841309\n",
      "  batch 6250 Pearson correlation: 0.9569743560520353, Spearman correlation: 0.934986891169002\n",
      "  batch 6500 loss: 390.95284924316405\n",
      "  batch 6500 Pearson correlation: 0.9579924496936906, Spearman correlation: 0.936259820258929\n",
      "  batch 6750 loss: 375.5547801208496\n",
      "  batch 6750 Pearson correlation: 0.9599995078832589, Spearman correlation: 0.9380919667369886\n",
      "  batch 7000 loss: 379.8480502319336\n",
      "  batch 7000 Pearson correlation: 0.9589613648929806, Spearman correlation: 0.9386083193861443\n",
      "  batch 7250 loss: 408.19303649902344\n",
      "  batch 7250 Pearson correlation: 0.957511130959118, Spearman correlation: 0.9311725479867187\n",
      "  batch 7500 loss: nan\n",
      "  batch 7500 Pearson correlation: 0.9571600694401909, Spearman correlation: 0.9358450436108674\n",
      "  batch 7750 loss: 387.35587286376955\n",
      "  batch 7750 Pearson correlation: 0.9583417401537289, Spearman correlation: 0.9367741297596226\n",
      "  batch 8000 loss: 378.72787783813476\n",
      "  batch 8000 Pearson correlation: 0.9590503933547682, Spearman correlation: 0.9377505343918497\n",
      "  batch 8250 loss: 403.0681408691406\n",
      "  batch 8250 Pearson correlation: 0.9566157467047808, Spearman correlation: 0.9339953518775851\n",
      "  batch 8500 loss: 388.38688714599607\n",
      "  batch 8500 Pearson correlation: 0.9586229431746417, Spearman correlation: 0.9377945535438158\n",
      "LOSS train 388.38688714599607 valid 490.5002746582031\n",
      "Validation Pearson Correlation: 0.9511629352570938, Spearman Correlation: 0.9232564989520374\n",
      "EPOCH 4\n",
      "-------------------------------\n",
      "  batch 250 loss: 381.1381679992676\n",
      "  batch 250 Pearson correlation: 0.9588197024956719, Spearman correlation: 0.9357971149952033\n",
      "  batch 500 loss: 362.5553101501465\n",
      "  batch 500 Pearson correlation: 0.9603062183521169, Spearman correlation: 0.9357984580576274\n",
      "  batch 750 loss: 411.4513303527832\n",
      "  batch 750 Pearson correlation: 0.9561566267042034, Spearman correlation: 0.9325863149221965\n",
      "  batch 1000 loss: 408.7533726806641\n",
      "  batch 1000 Pearson correlation: 0.9560332606321564, Spearman correlation: 0.9339509483167759\n",
      "  batch 1250 loss: 376.44517749023436\n",
      "  batch 1250 Pearson correlation: 0.9592143891031364, Spearman correlation: 0.9373853474355701\n",
      "  batch 1500 loss: 375.60163232421877\n",
      "  batch 1500 Pearson correlation: 0.9598317323185316, Spearman correlation: 0.9394734722521724\n",
      "  batch 1750 loss: 381.50244204711913\n",
      "  batch 1750 Pearson correlation: 0.9589976320147685, Spearman correlation: 0.9370372239050078\n",
      "  batch 2000 loss: 379.7494545593262\n",
      "  batch 2000 Pearson correlation: 0.9593622512825765, Spearman correlation: 0.9365143381147715\n",
      "  batch 2250 loss: 392.72869717407224\n",
      "  batch 2250 Pearson correlation: 0.9579774210883303, Spearman correlation: 0.9356408092653223\n",
      "  batch 2500 loss: 386.0025194091797\n",
      "  batch 2500 Pearson correlation: 0.9582251026899519, Spearman correlation: 0.9385726308920572\n",
      "  batch 2750 loss: 376.0917663269043\n",
      "  batch 2750 Pearson correlation: 0.9600036482725639, Spearman correlation: 0.9378087733865383\n",
      "  batch 3000 loss: 375.435416015625\n",
      "  batch 3000 Pearson correlation: 0.95983164481901, Spearman correlation: 0.9403168744512371\n",
      "  batch 3250 loss: 386.683510345459\n",
      "  batch 3250 Pearson correlation: 0.9589229769887452, Spearman correlation: 0.9366394609586238\n",
      "  batch 3500 loss: 386.75599935913084\n",
      "  batch 3500 Pearson correlation: 0.9596216560002127, Spearman correlation: 0.9387977230537888\n",
      "  batch 3750 loss: 386.2521522216797\n",
      "  batch 3750 Pearson correlation: 0.9586839080918251, Spearman correlation: 0.9370294102226175\n",
      "  batch 4000 loss: 367.8220372619629\n",
      "  batch 4000 Pearson correlation: 0.9606583911302451, Spearman correlation: 0.9415583972888033\n",
      "  batch 4250 loss: 392.58617166137697\n",
      "  batch 4250 Pearson correlation: 0.9576329731382943, Spearman correlation: 0.9360545016739524\n",
      "  batch 4500 loss: 385.91562533569333\n",
      "  batch 4500 Pearson correlation: 0.9590175092388215, Spearman correlation: 0.9384842430959721\n",
      "  batch 4750 loss: 394.6548533325195\n",
      "  batch 4750 Pearson correlation: 0.9581310291091001, Spearman correlation: 0.9358441349782476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 5000 loss: 384.2023699645996\n",
      "  batch 5000 Pearson correlation: 0.9586742654038378, Spearman correlation: 0.9385369600721981\n",
      "  batch 5250 loss: 374.3661752319336\n",
      "  batch 5250 Pearson correlation: 0.9602778612916012, Spearman correlation: 0.9407236252469822\n",
      "  batch 5500 loss: 368.11929946899414\n",
      "  batch 5500 Pearson correlation: 0.9595254005474174, Spearman correlation: 0.9398692500952738\n",
      "  batch 5750 loss: 396.49502029418943\n",
      "  batch 5750 Pearson correlation: 0.9576599945380139, Spearman correlation: 0.9365927927540821\n",
      "  batch 6000 loss: 385.8930966796875\n",
      "  batch 6000 Pearson correlation: 0.9590336619880777, Spearman correlation: 0.9370725052974254\n",
      "  batch 6250 loss: 385.945269317627\n",
      "  batch 6250 Pearson correlation: 0.9582135046574213, Spearman correlation: 0.9344298509529265\n",
      "  batch 6500 loss: 372.2580586395264\n",
      "  batch 6500 Pearson correlation: 0.9596703167882401, Spearman correlation: 0.9405581461749425\n",
      "  batch 6750 loss: 386.50347763061524\n",
      "  batch 6750 Pearson correlation: 0.9591376781212185, Spearman correlation: 0.938169778810088\n",
      "  batch 7000 loss: 380.40350955200194\n",
      "  batch 7000 Pearson correlation: 0.9592800613011212, Spearman correlation: 0.9395413942296362\n",
      "  batch 7250 loss: 405.60026257324216\n",
      "  batch 7250 Pearson correlation: 0.9565794848217627, Spearman correlation: 0.9326398882399546\n",
      "  batch 7500 loss: 398.6931413879395\n",
      "  batch 7500 Pearson correlation: 0.9569415211069333, Spearman correlation: 0.9326700129093388\n",
      "  batch 7750 loss: 381.4878104248047\n",
      "  batch 7750 Pearson correlation: 0.9595751274584611, Spearman correlation: 0.9384678829206458\n",
      "  batch 8000 loss: nan\n",
      "  batch 8000 Pearson correlation: 0.9578229125889499, Spearman correlation: 0.9388081845970268\n",
      "  batch 8250 loss: 374.5197684326172\n",
      "  batch 8250 Pearson correlation: 0.9596328534047129, Spearman correlation: 0.9382897253584894\n",
      "  batch 8500 loss: 384.1062367248535\n",
      "  batch 8500 Pearson correlation: 0.9584448490706527, Spearman correlation: 0.9347611876273085\n",
      "LOSS train 384.1062367248535 valid 506.590576171875\n",
      "Validation Pearson Correlation: 0.949536516635921, Spearman Correlation: 0.9224681471625946\n",
      "EPOCH 5\n",
      "-------------------------------\n",
      "  batch 250 loss: 359.841835144043\n",
      "  batch 250 Pearson correlation: 0.961088230017817, Spearman correlation: 0.9397646588964275\n",
      "  batch 500 loss: 382.39690185546874\n",
      "  batch 500 Pearson correlation: 0.9591119062314709, Spearman correlation: 0.9370680422380196\n",
      "  batch 750 loss: 380.5415823059082\n",
      "  batch 750 Pearson correlation: 0.9595579587404137, Spearman correlation: 0.9383891178038025\n",
      "  batch 1000 loss: 372.0281896057129\n",
      "  batch 1000 Pearson correlation: 0.9599613665375278, Spearman correlation: 0.9368079938891073\n",
      "  batch 1250 loss: nan\n",
      "  batch 1250 Pearson correlation: 0.9598715009153374, Spearman correlation: 0.9397271471263637\n",
      "  batch 1500 loss: 373.9467360839844\n",
      "  batch 1500 Pearson correlation: 0.9597468371222405, Spearman correlation: 0.9379990876477164\n",
      "  batch 1750 loss: 381.40411114501956\n",
      "  batch 1750 Pearson correlation: 0.959491924377734, Spearman correlation: 0.9364889745146513\n",
      "  batch 2000 loss: 369.6578394165039\n",
      "  batch 2000 Pearson correlation: 0.9599095572746622, Spearman correlation: 0.9386765676102933\n",
      "  batch 2250 loss: 367.57466119384765\n",
      "  batch 2250 Pearson correlation: 0.9603095429804086, Spearman correlation: 0.9351198900401857\n",
      "  batch 2500 loss: 379.1873860473633\n",
      "  batch 2500 Pearson correlation: 0.9598119378334563, Spearman correlation: 0.9385871918277503\n",
      "  batch 2750 loss: 364.99227017211916\n",
      "  batch 2750 Pearson correlation: 0.960731168784448, Spearman correlation: 0.9425593835244142\n",
      "  batch 3000 loss: 372.6034315338135\n",
      "  batch 3000 Pearson correlation: 0.960159763637087, Spearman correlation: 0.9393478437485518\n",
      "  batch 3250 loss: 373.11117321777346\n",
      "  batch 3250 Pearson correlation: 0.9600562006303078, Spearman correlation: 0.93918861134726\n",
      "  batch 3500 loss: 371.80796389770506\n",
      "  batch 3500 Pearson correlation: 0.960002003272397, Spearman correlation: 0.9372295162721529\n",
      "  batch 3750 loss: 385.6993955078125\n",
      "  batch 3750 Pearson correlation: 0.9592400790284036, Spearman correlation: 0.9395142816546554\n",
      "  batch 4000 loss: 373.65818594360354\n",
      "  batch 4000 Pearson correlation: 0.9606130320024368, Spearman correlation: 0.9424464480492555\n",
      "  batch 4250 loss: 386.4857541503906\n",
      "  batch 4250 Pearson correlation: 0.9589246144579378, Spearman correlation: 0.9366447863168189\n",
      "  batch 4500 loss: 366.67177740478513\n",
      "  batch 4500 Pearson correlation: 0.9605410162425474, Spearman correlation: 0.9415529622347228\n",
      "  batch 4750 loss: 371.57152374267577\n",
      "  batch 4750 Pearson correlation: 0.9597997247497314, Spearman correlation: 0.938580938496506\n",
      "  batch 5000 loss: 370.2290289306641\n",
      "  batch 5000 Pearson correlation: 0.9593672172335049, Spearman correlation: 0.9394979981688109\n",
      "  batch 5250 loss: 368.59601016235354\n",
      "  batch 5250 Pearson correlation: 0.960950715119645, Spearman correlation: 0.9392975359152856\n",
      "  batch 5500 loss: 359.756327331543\n",
      "  batch 5500 Pearson correlation: 0.960782078756178, Spearman correlation: 0.9402650927105214\n",
      "  batch 5750 loss: 383.90756536865234\n",
      "  batch 5750 Pearson correlation: 0.9590499900520651, Spearman correlation: 0.9392148120195363\n",
      "  batch 6000 loss: 368.68569360351563\n",
      "  batch 6000 Pearson correlation: 0.960482594813397, Spearman correlation: 0.940925619463891\n",
      "  batch 6250 loss: 380.6148518676758\n",
      "  batch 6250 Pearson correlation: 0.9593816802167242, Spearman correlation: 0.9394694594431675\n",
      "  batch 6500 loss: 364.19638342285157\n",
      "  batch 6500 Pearson correlation: 0.9610029925472089, Spearman correlation: 0.9401116387414153\n",
      "  batch 6750 loss: 382.5856730957031\n",
      "  batch 6750 Pearson correlation: 0.9588077424110452, Spearman correlation: 0.9367911499999356\n",
      "  batch 7000 loss: 383.0527194519043\n",
      "  batch 7000 Pearson correlation: 0.9587184974407343, Spearman correlation: 0.936963304796547\n",
      "  batch 7250 loss: 393.60569668579103\n",
      "  batch 7250 Pearson correlation: 0.9585169623913354, Spearman correlation: 0.9374065673559241\n",
      "  batch 7500 loss: 355.9330903015137\n",
      "  batch 7500 Pearson correlation: 0.9622201245417613, Spearman correlation: 0.9437128730672775\n",
      "  batch 7750 loss: 380.12797576904296\n",
      "  batch 7750 Pearson correlation: 0.959491569653651, Spearman correlation: 0.9370128509228225\n",
      "  batch 8000 loss: 385.4834169006348\n",
      "  batch 8000 Pearson correlation: 0.959052801596977, Spearman correlation: 0.9383225498664387\n",
      "  batch 8250 loss: 369.1796115722656\n",
      "  batch 8250 Pearson correlation: 0.9608338974021648, Spearman correlation: 0.9419724527869647\n",
      "  batch 8500 loss: 389.0245927734375\n",
      "  batch 8500 Pearson correlation: 0.9575144664872804, Spearman correlation: 0.9352745237659411\n",
      "LOSS train 389.0245927734375 valid 503.8782653808594\n",
      "Validation Pearson Correlation: 0.950540025909915, Spearman Correlation: 0.9233459645533928\n"
     ]
    }
   ],
   "source": [
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/mlp_model_{}'.format(timestamp))\n",
    "\n",
    "epoch_number = 0\n",
    "EPOCHS = 5\n",
    "best_vloss = 1_000_000.\n",
    "\n",
    "epoch_loss, epoch_coeff = [], []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}\\n-------------------------------'.format(epoch_number + 1))\n",
    "\n",
    "    curr_model.train(True)\n",
    "    avg_loss,pcorr,scorr = train_one_epoch(epoch_number, writer)\n",
    "    running_vloss = 0.0\n",
    "    curr_model.eval()\n",
    "    \n",
    "    all_vpredictions = []\n",
    "    all_vlabels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, vdata in enumerate(validation_loader):\n",
    "            vinputs, vlabels = vdata\n",
    "            voutputs = curr_model(vinputs.transpose(1,2))\n",
    "            vloss = loss_fn(voutputs, vlabels.squeeze(1))\n",
    "            running_vloss += vloss\n",
    "            \n",
    "            all_vpredictions.append(voutputs)\n",
    "            all_vlabels.append(vlabels)\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    \n",
    "    predictions_flat = torch.cat(all_vpredictions).view(-1)\n",
    "    labels_flat = torch.cat(all_vlabels).view(-1)\n",
    "    pval_corr = pearson_correlation(predictions_flat, labels_flat)\n",
    "    sval_corr = spearman_correlation(predictions_flat,labels_flat)\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "    print('Validation Pearson Correlation: {}, Spearman Correlation: {}'.format(pval_corr.item(),sval_corr.item()))\n",
    "    \n",
    "    writer.add_scalars('Training vs. Validation Loss',\n",
    "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                    epoch_number + 1)\n",
    "    writer.flush()\n",
    "\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n",
    "        torch.save(curr_model.state_dict(), model_path)\n",
    "\n",
    "    epoch_loss.append((avg_loss, avg_vloss))\n",
    "    epoch_coeff.append((scorr, sval_corr))\n",
    "    \n",
    "    epoch_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ee4a16a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.8023441110268037, 0.8198872713965447), (0.8579737297804155, 0.8433671005928176), (0.8603200804955299, 0.8590490294150954), (0.8825058004687943, 0.8827406891320867), (0.8728390414949895, 0.8743633067935532)]\n"
     ]
    }
   ],
   "source": [
    "print(epoch_coeff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407aca05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_cs282a_mlp",
   "language": "python",
   "name": "pytorch_cs282a_mlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
